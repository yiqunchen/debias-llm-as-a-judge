[{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 Chen Lab Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/continuous-outcomes.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Continuous Outcome Example","text":"Getting Started vignette focuses discrete outcomes (e.g., binary win/loss classifications), many LLM evaluation tasks involve continuous scores. example: Quality scores 1-10 scale Helpfulness ratings Relevance scores retrieval Fluency coherence metrics LLM judge produces continuous predictions \\(\\hat{Y}\\) true continuous score \\(Y\\), naive approach averaging \\(\\hat{Y}\\) can biased LLM’s predictions systematically . vignette demonstrates correct bias using calibration data.","code":""},{"path":[]},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/continuous-outcomes.html","id":"the-goal","dir":"Articles","previous_headings":"Problem Setup","what":"The Goal","title":"Continuous Outcome Example","text":"want estimate population mean true continuous score: \\[\\theta = E[Y]\\] \\(Y\\) ground truth score (e.g., human ratings) \\(\\hat{Y}\\) LLM judge’s prediction.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/continuous-outcomes.html","id":"why-bias-occurs","dir":"Articles","previous_headings":"Problem Setup","what":"Why Bias Occurs","title":"Continuous Outcome Example","text":"continuous outcomes, bias arises : Systematic /-prediction: LLM consistently rates higher lower humans Non-linear calibration: relationship \\(\\hat{Y}\\) \\(Y\\) one--one Distribution shift: test data differs training data prediction model Unlike discrete case model sensitivity/specificity, model conditional expectation \\(E[Y | \\hat{Y}]\\) using various calibration methods.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/continuous-outcomes.html","id":"available-estimators","dir":"Articles","previous_headings":"Problem Setup","what":"Available Estimators","title":"Continuous Outcome Example","text":"continuous outcomes, package provides:","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/continuous-outcomes.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Continuous Outcome Example","text":"","code":"library(debiasLLMReporting) set.seed(42)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/continuous-outcomes.html","id":"simulated-example","dir":"Articles","previous_headings":"","what":"Simulated Example","title":"Continuous Outcome Example","text":"Let’s simulate scenario LLM judge provides quality scores, systematic bias.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/continuous-outcomes.html","id":"generate-data","dir":"Articles","previous_headings":"Simulated Example","what":"Generate Data","title":"Continuous Outcome Example","text":"","code":"# Parameters N <- 500          # Total samples m <- 50           # Calibration set size (10% labeled)  # Generate true scores (e.g., human quality ratings on 1-10 scale) # True mean is around 7, but with variation Y_all <- rnorm(N, mean = 7.0, sd = 1.5)  # LLM predictions: regress toward the mean (less extreme than truth) # Yhat = 0.7 * Y + 0.3 * 5 + noise # The LLM \"pulls\" predictions toward 5 (middle of 1-10 scale) shrinkage <- 0.7 anchor <- 5.0  # LLM's implicit \"anchor\" point Yhat_all <- shrinkage * Y_all + (1 - shrinkage) * anchor + rnorm(N, 0, 0.4)  # Split into calibration and test sets idx_cal <- sample(N, m) idx_test <- setdiff(seq_len(N), idx_cal)  Y_cal <- Y_all[idx_cal] Yhat_cal <- Yhat_all[idx_cal] Yhat_test <- Yhat_all[idx_test] Y_test <- Y_all[idx_test]  # Ground truth for test (only for evaluation)  # True mean on test set (what we want to estimate) theta_test <- mean(Y_test) cat(\"True mean score (test set):\", round(theta_test, 3), \"\\n\") #> True mean score (test set): 6.913 cat(\"Naive estimate (mean of Yhat):\", round(mean(Yhat_test), 3), \"\\n\") #> Naive estimate (mean of Yhat): 6.332 cat(\"Bias in naive estimate:\", round(mean(Yhat_test) - theta_test, 3), \"\\n\") #> Bias in naive estimate: -0.581"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/continuous-outcomes.html","id":"visualize-calibration-data","dir":"Articles","previous_headings":"Simulated Example","what":"Visualize Calibration Data","title":"Continuous Outcome Example","text":"","code":"library(ggplot2) #> Warning: package 'ggplot2' was built under R version 4.4.3  cal_df <- data.frame(Y = Y_cal, Yhat = Yhat_cal)  ggplot(cal_df, aes(x = Yhat, y = Y)) +   geom_point(alpha = 0.6) +   geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray50\") +   geom_smooth(method = \"lm\", se = TRUE, color = \"#E6AB02\") +   labs(     title = \"Calibration Data: LLM Predictions vs Human Scores\",     subtitle = \"Dashed line = perfect calibration\",     x = \"LLM Prediction (Yhat)\",     y = \"Human Score (Y)\"   ) +   theme_bw() +   coord_fixed(xlim = c(1, 10), ylim = c(1, 10)) #> `geom_smooth()` using formula = 'y ~ x'"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/continuous-outcomes.html","id":"apply-estimators","dir":"Articles","previous_headings":"Simulated Example","what":"Apply Estimators","title":"Continuous Outcome Example","text":"","code":"# 1. Naive estimate (no correction) naive_result <- naive_continuous_point_and_ci(   Yhat_test = Yhat_test,   alpha = 0.10 )  # 2. PPI (Prediction-Powered Inference) ppi_result <- ppi_continuous_point_and_ci(   Y_cal = Y_cal,   Yhat_cal = Yhat_cal,   Yhat_test = Yhat_test,   alpha = 0.10 )  # 3. PPI++ (with optimized lambda) ppi_pp_result <- ppi_pp_continuous_point_and_ci(   Y_cal = Y_cal,   Yhat_cal = Yhat_cal,   Yhat_test = Yhat_test,   alpha = 0.10 )  # 4. EIF with linear calibration eif_linear_result <- eif_continuous_point_and_ci(   Y_cal = Y_cal,   Yhat_cal = Yhat_cal,   Yhat_test = Yhat_test,   calibration_method = \"linear\",   alpha = 0.10 )  # 5. EIF with GAM calibration (more flexible) eif_gam_result <- eif_continuous_point_and_ci(   Y_cal = Y_cal,   Yhat_cal = Yhat_cal,   Yhat_test = Yhat_test,   calibration_method = \"gam\",   alpha = 0.10 )  # 6. EIF with isotonic calibration (monotonic) eif_iso_result <- eif_continuous_point_and_ci(   Y_cal = Y_cal,   Yhat_cal = Yhat_cal,   Yhat_test = Yhat_test,   calibration_method = \"isotonic\",   alpha = 0.10 )"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/continuous-outcomes.html","id":"compare-results","dir":"Articles","previous_headings":"Simulated Example","what":"Compare Results","title":"Continuous Outcome Example","text":"","code":"results <- data.frame(   Method = c(\"True\", \"Naive\", \"PPI\", \"PPI++\",              \"EIF (Linear)\", \"EIF (GAM)\", \"EIF (Isotonic)\"),   Estimate = c(     theta_test,     naive_result$theta,     ppi_result$theta,     ppi_pp_result$theta,     eif_linear_result$theta,     eif_gam_result$theta,     eif_iso_result$theta   ),   CI_Lower = c(     NA,     naive_result$ci_lower,     ppi_result$ci_lower,     ppi_pp_result$ci_lower,     eif_linear_result$ci_lower,     eif_gam_result$ci_lower,     eif_iso_result$ci_lower   ),   CI_Upper = c(     NA,     naive_result$ci_upper,     ppi_result$ci_upper,     ppi_pp_result$ci_upper,     eif_linear_result$ci_upper,     eif_gam_result$ci_upper,     eif_iso_result$ci_upper   ) )  results$Bias <- results$Estimate - theta_test results$CI_Width <- results$CI_Upper - results$CI_Lower results$Covers_True <- results$CI_Lower <= theta_test & results$CI_Upper >= theta_test  knitr::kable(results, digits = 3)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/continuous-outcomes.html","id":"visualize-results","dir":"Articles","previous_headings":"Simulated Example","what":"Visualize Results","title":"Continuous Outcome Example","text":"","code":"plot_df <- results[results$Method != \"True\", ] plot_df$Method <- factor(plot_df$Method,                          levels = c(\"Naive\", \"PPI\", \"PPI++\",                                     \"EIF (Linear)\", \"EIF (GAM)\", \"EIF (Isotonic)\"))  ggplot(plot_df, aes(x = Method, y = Estimate, color = Method)) +   geom_point(size = 3) +   geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper), width = 0.2, linewidth = 1) +   geom_hline(yintercept = theta_test, linetype = \"dashed\", linewidth = 1) +   annotate(\"text\", x = 0.6, y = theta_test + 0.15,            label = paste(\"True =\", round(theta_test, 2)), hjust = 0) +   scale_color_manual(values = c(     \"Naive\" = \"#525252\",     \"PPI\" = \"#1B9E77\",     \"PPI++\" = \"#D95F02\",     \"EIF (Linear)\" = \"#7570B3\",     \"EIF (GAM)\" = \"#E6AB02\",     \"EIF (Isotonic)\" = \"#E7298A\"   )) +   labs(     title = \"Mean Score Estimation with Bias Correction\",     subtitle = paste0(\"N = \", N, \", m = \", m, \" (\", round(100*m/N), \"% labeled)\"),     y = \"Estimated Mean Score\",     x = NULL   ) +   theme_bw() +   theme(     legend.position = \"none\",     axis.text.x = element_text(angle = 30, hjust = 1)   )"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/continuous-outcomes.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next Steps","title":"Continuous Outcome Example","text":"See Getting Started vignette discrete outcome examples See Real Data Example applying methods actual LLM evaluations","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/getting-started.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Getting Started with debiasLLMReporting","text":"using LLMs judges evaluate models (e.g., comparing outputs pairwise comparisons), LLM judge introduces measurement error. package provides methods correct bias using small calibration sample human labels LLM predictions available. key insight confusion matrix estimated calibration data, can “debias” prevalence estimate full (unlabeled) test set.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/getting-started.html","id":"problem-setup","dir":"Articles","previous_headings":"","what":"Problem Setup","title":"Getting Started with debiasLLMReporting","text":"Consider task estimating win rate Model versus Model B based pairwise comparisons. ground truth determined human evaluators, human labeling expensive. Instead, use LLM judge evaluate large number comparisons.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/getting-started.html","id":"the-challenge","dir":"Articles","previous_headings":"Problem Setup","what":"The Challenge","title":"Getting Started with debiasLLMReporting","text":"Let \\(Y \\\\{0, 1\\}\\) denote true human preference (1 = Model wins) \\(\\hat{Y} \\\\{0, 1\\}\\) denote LLM judge’s prediction. goal estimate: \\[\\theta = P(Y = 1) = \\text{true win rate Model }\\] naive approach use \\(\\hat{\\theta}_{\\text{naive}} = \\frac{1}{n}\\sum_{=1}^{n} \\hat{Y}_i\\), biased LLM judge makes errors.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/getting-started.html","id":"misclassification-model","dir":"Articles","previous_headings":"Problem Setup","what":"Misclassification Model","title":"Getting Started with debiasLLMReporting","text":"model LLM judge’s errors using two parameters: Sensitivity \\(q_1 = P(\\hat{Y} = 1 \\mid Y = 1)\\): probability judge correctly identifies Model wins Specificity \\(q_0 = P(\\hat{Y} = 0 \\mid Y = 0)\\): probability judge correctly identifies Model B wins \\(q_0, q_1 < 1\\), naive estimator biased. example, judge preference Model (high \\(q_1\\), low \\(q_0\\)), naive estimate overestimate Model ’s true win rate.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/getting-started.html","id":"the-solution-calibration-correction","dir":"Articles","previous_headings":"Problem Setup","what":"The Solution: Calibration + Correction","title":"Getting Started with debiasLLMReporting","text":"assume access : Calibration set (\\(m\\) samples): human labels \\(Y\\) judge predictions \\(\\hat{Y}\\) observed Test set (\\(n\\) samples): judge predictions \\(\\hat{Y}\\) observed calibration set, estimate \\(q_0\\) \\(q_1\\). apply correction methods obtain unbiased estimate \\(\\theta\\) valid confidence intervals.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/getting-started.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Getting Started with debiasLLMReporting","text":"","code":"library(debiasLLMReporting) set.seed(2026)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/getting-started.html","id":"simulated-example","dir":"Articles","previous_headings":"","what":"Simulated Example","title":"Getting Started with debiasLLMReporting","text":"Let’s simulate scenario : want estimate true win rate Model vs Model B LLM judge provides predictions, imperfect accuracy small calibration set human labels","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/getting-started.html","id":"generate-data","dir":"Articles","previous_headings":"Simulated Example","what":"Generate Data","title":"Getting Started with debiasLLMReporting","text":"","code":"# Parameters N <- 500          # Total samples m <- 50           # Calibration set size (10% labeled) theta_true <- 0.6 # True win rate for Model A q0 <- 0.85        # Specificity: P(judge says B | human says B) q1 <- 0.80        # Sensitivity: P(judge says A | human says A)  # Generate true labels Y_all <- rbinom(N, 1, theta_true)  # Generate LLM judge predictions (with misclassification) Yhat_all <- ifelse(   Y_all == 1,   rbinom(N, 1, q1),       # True positives detected with prob q1   1 - rbinom(N, 1, q0)    # True negatives detected with prob q0 )  # Split into calibration and test sets idx_cal <- sample(N, m) idx_test <- setdiff(seq_len(N), idx_cal)  Y_cal <- Y_all[idx_cal] Yhat_cal <- Yhat_all[idx_cal] Yhat_test <- Yhat_all[idx_test]"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/getting-started.html","id":"apply-estimators","dir":"Articles","previous_headings":"Simulated Example","what":"Apply Estimators","title":"Getting Started with debiasLLMReporting","text":"","code":"# 1. Naive estimate (no correction) naive_est <- mean(Yhat_test) cat(\"Naive estimate:\", round(naive_est, 3), \"\\n\") #> Naive estimate: 0.496  # 2. PPI (Prediction-Powered Inference) ppi_result <- ppi_point_and_ci(   Y_L = Y_cal,   f_L = Yhat_cal,   f_U = Yhat_test,   alpha = 0.10 )  # 3. PPI++ (with optimized lambda) ppi_pp_result <- ppi_pp_point_and_ci(   Y_L = Y_cal,   f_L = Yhat_cal,   f_U = Yhat_test,   alpha = 0.10 )  # 4. Rogan-Gladen (classical measurement error correction) m0 <- sum(Y_cal == 0) m1 <- sum(Y_cal == 1) q0_hat <- mean(Yhat_cal[Y_cal == 0] == 0) q1_hat <- mean(Yhat_cal[Y_cal == 1] == 1) p_hat <- mean(Yhat_test)  rg_result <- rg_point_and_ci(   p_hat = p_hat,   q0_hat = q0_hat,   q1_hat = q1_hat,   n = length(Yhat_test),   m0 = m0,   m1 = m1,   alpha = 0.10 )  # 5. EIF (Efficient Influence Function) eif_result <- eif_point_and_ci(   Y_cal = Y_cal,   Yhat_cal = Yhat_cal,   Yhat_test = Yhat_test,   alpha = 0.10 )  # 6. MLE (Joint Maximum Likelihood) mle_result <- mle_point_and_ci(   Y_cal = Y_cal,   Yhat_cal = Yhat_cal,   Yhat_test = Yhat_test,   alpha = 0.10 )"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/getting-started.html","id":"compare-results","dir":"Articles","previous_headings":"Simulated Example","what":"Compare Results","title":"Getting Started with debiasLLMReporting","text":"","code":"results <- data.frame(   Method = c(\"True\", \"Naive\", \"PPI\", \"PPI++\", \"Rogan-Gladen\", \"EIF\", \"MLE\"),   Estimate = c(     theta_true,     naive_est,     ppi_result$theta,     ppi_pp_result$theta,     rg_result$theta,     eif_result$theta,     mle_result$theta   ),   CI_Lower = c(NA, NA, ppi_result$ci_lower, ppi_pp_result$ci_lower, rg_result$ci_lower,                eif_result$ci_lower, mle_result$ci_lower),   CI_Upper = c(NA, NA, ppi_result$ci_upper, ppi_pp_result$ci_upper, rg_result$ci_upper,                eif_result$ci_upper, mle_result$ci_upper) )  results$Bias <- results$Estimate - theta_true results$CI_Width <- results$CI_Upper - results$CI_Lower results$Covers_True <- results$CI_Lower <= theta_true & results$CI_Upper >= theta_true  knitr::kable(results, digits = 3)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/getting-started.html","id":"visualize","dir":"Articles","previous_headings":"Simulated Example","what":"Visualize","title":"Getting Started with debiasLLMReporting","text":"","code":"library(ggplot2) #> Warning: package 'ggplot2' was built under R version 4.4.3  plot_df <- results[results$Method != \"True\", ] plot_df$Method <- factor(plot_df$Method,                          levels = c(\"Naive\", \"PPI\", \"PPI++\", \"Rogan-Gladen\", \"EIF\", \"MLE\"))  ggplot(plot_df, aes(x = Method, y = Estimate, color = Method)) +    geom_point(size = 3) +   geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper), width = 0.2, linewidth = 1,                 na.rm = TRUE) +   geom_hline(yintercept = theta_true, linetype = \"dashed\", linewidth = 1) +   annotate(\"text\", x = 0.6, y = theta_true + 0.03,            label = paste(\"True =\", theta_true), hjust = 0) +   scale_color_manual(values = c(     \"Naive\" = \"#525252\",     \"PPI\" = \"#1B9E77\",     \"PPI++\" = \"#D95F02\",     \"Rogan-Gladen\" = \"#7570B3\",     \"EIF\" = \"#E6AB02\",     \"MLE\" = \"#E7298A\"   )) +   labs(     title = \"Win Rate Estimation with Measurement Error Correction\",     subtitle = paste0(\"N = \", N, \", m = \", m, \" (\", round(100*m/N), \"% labeled)\"),     y = \"Estimated Win Rate\",     x = NULL   ) +   theme_bw() +   theme(legend.position = \"none\") +   coord_cartesian(ylim = c(0, 1))"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/getting-started.html","id":"key-takeaways","dir":"Articles","previous_headings":"","what":"Key Takeaways","title":"Getting Started with debiasLLMReporting","text":"Naive estimates biased LLM judge imperfect accuracy PPI uses calibration residuals correct bias PPI++ adaptively weights correction minimize variance Rogan-Gladen uses classical epidemiological approach based sensitivity/specificity EIF achieves semiparametric efficiency using efficient influence function MLE jointly estimates prevalence, sensitivity, specificity via maximum likelihood corrected methods aim provide valid confidence intervals nominal coverage, naive estimate’s CI cover true value ’s systematic bias.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/getting-started.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next Steps","title":"Getting Started with debiasLLMReporting","text":"See Continuous Outcomes vignette handling continuous scores instead binary outcomes See Real Data Example vignette applying methods actual LLM judge evaluations","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/real-data-example.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Real Data Example: GPT-4o-mini as Judge","text":"vignette demonstrates applying debiasing methods real LLM---judge data. use GPT-4o-mini judge evaluate pairwise comparisons Claude Opus 4 Gemini 2.5 Pro, comparing judge decisions human preferences Chatbot Arena.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/real-data-example.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Real Data Example: GPT-4o-mini as Judge","text":"","code":"library(debiasLLMReporting) library(jsonlite) library(dplyr) library(ggplot2) #> Warning: package 'ggplot2' was built under R version 4.4.3  set.seed(2026)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/real-data-example.html","id":"load-data","dir":"Articles","previous_headings":"","what":"Load Data","title":"Real Data Example: GPT-4o-mini as Judge","text":"package includes example judge results GPT-4o-mini evaluating Claude Opus 4 vs Gemini 2.5 Pro comparisons:","code":"# Load from package extdata json_path <- system.file(\"extdata\", \"example_judge_data.json\",                          package = \"debiasLLMReporting\") judge_data <- fromJSON(json_path)  head(judge_data) #>                                     id                model_a #> 1 71fdece3-2416-4f9c-ad22-17d759811c9d claude-opus-4-20250514 #> 2 8e4e7918-3754-4c72-bd02-e99e67122f32         gemini-2.5-pro #> 3 f582a2c0-668b-47a3-8de7-153c57fde3f5 claude-opus-4-20250514 #> 4 d28f59c3-66fb-49fe-8b2b-00f3d6e853b3 claude-opus-4-20250514 #> 5 3bb70211-d6b1-463b-a785-79dff5a338d8         gemini-2.5-pro #> 6 2e928f7f-e1a9-4b7d-8287-cce661d8f1fb claude-opus-4-20250514 #>                  model_b human_winner judge_pick error #> 1         gemini-2.5-pro      model_b          B  <NA> #> 2 claude-opus-4-20250514      model_a          A  <NA> #> 3         gemini-2.5-pro      model_a          A  <NA> #> 4         gemini-2.5-pro      model_b          B  <NA> #> 5 claude-opus-4-20250514      model_a          A  <NA> #> 6         gemini-2.5-pro      model_b          B  <NA>"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/real-data-example.html","id":"prepare-data","dir":"Articles","previous_headings":"","what":"Prepare Data","title":"Real Data Example: GPT-4o-mini as Judge","text":"data contains pairwise comparisons model positions (vs B) randomized. need normalize consistently measure Claude’s win rate:","code":"df <- judge_data %>%   filter(!is.na(judge_pick)) %>%   mutate(     # Identify which position Claude is in     claude_is_a = (model_a == \"claude-opus-4-20250514\"),     # Y = 1 if human preferred Claude, 0 otherwise     Y = case_when(       claude_is_a & human_winner == \"model_a\" ~ 1L,       !claude_is_a & human_winner == \"model_b\" ~ 1L,       TRUE ~ 0L     ),     # Yhat = 1 if judge picked Claude, 0 otherwise     Yhat = case_when(       claude_is_a & judge_pick == \"A\" ~ 1L,       !claude_is_a & judge_pick == \"B\" ~ 1L,       TRUE ~ 0L     )   )  cat(\"Total comparisons:\", nrow(df), \"\\n\") #> Total comparisons: 276 cat(\"Human preference for Claude:\", round(mean(df$Y), 3), \"\\n\") #> Human preference for Claude: 0.236 cat(\"Judge preference for Claude:\", round(mean(df$Yhat), 3), \"\\n\") #> Judge preference for Claude: 0.159"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/real-data-example.html","id":"split-into-calibration-and-test-sets","dir":"Articles","previous_headings":"","what":"Split into Calibration and Test Sets","title":"Real Data Example: GPT-4o-mini as Judge","text":"use 10% data labeled calibration set (human judge labels). practice, calibration set require expensive human annotation.","code":"LABEL_RATIO <- 0.10 N <- nrow(df) m <- max(4L, round(LABEL_RATIO * N)) n <- N - m  # Random split idx_cal <- sample(N, m) idx_test <- setdiff(seq_len(N), idx_cal)  Y_cal <- df$Y[idx_cal] Yhat_cal <- df$Yhat[idx_cal] Yhat_test <- df$Yhat[idx_test]  cat(\"Calibration set size:\", m, \"(\", round(100*m/N, 1), \"% )\\n\") #> Calibration set size: 28 ( 10.1 % ) cat(\"Test set size:\", n, \"\\n\") #> Test set size: 248"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/real-data-example.html","id":"estimate-confusion-matrix","dir":"Articles","previous_headings":"","what":"Estimate Confusion Matrix","title":"Real Data Example: GPT-4o-mini as Judge","text":"calibration set, estimate judge’s sensitivity specificity:","code":"m0 <- sum(Y_cal == 0) m1 <- sum(Y_cal == 1)  # Specificity: P(judge says not-Claude | human says not-Claude) q0_hat <- mean(Yhat_cal[Y_cal == 0] == 0)  # Sensitivity: P(judge says Claude | human says Claude) q1_hat <- mean(Yhat_cal[Y_cal == 1] == 1)  # Positive rate in test set p_hat <- mean(Yhat_test)  cat(\"Estimated specificity (q0):\", round(q0_hat, 3), \"\\n\") #> Estimated specificity (q0): 0.875 cat(\"Estimated sensitivity (q1):\", round(q1_hat, 3), \"\\n\") #> Estimated sensitivity (q1): 0.25 cat(\"Test set positive rate:\", round(p_hat, 3), \"\\n\") #> Test set positive rate: 0.161"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/real-data-example.html","id":"apply-all-estimators","dir":"Articles","previous_headings":"","what":"Apply All Estimators","title":"Real Data Example: GPT-4o-mini as Judge","text":"","code":"ALPHA <- 0.10  # 90% confidence intervals  # Ground truth (using all human labels - in practice unknown for test set) theta_true <- mean(df$Y)  # 1. Naive (no correction) naive_theta <- p_hat naive_var <- p_hat * (1 - p_hat) / n z_alpha <- qnorm(1 - ALPHA / 2) naive_ci <- c(   pmax(naive_theta - z_alpha * sqrt(naive_var), 0),   pmin(naive_theta + z_alpha * sqrt(naive_var), 1) )  # 2. PPI ppi_result <- ppi_point_and_ci(   Y_L = Y_cal,   f_L = Yhat_cal,   f_U = Yhat_test,   alpha = ALPHA )  # 3. PPI++ ppi_pp_result <- ppi_pp_point_and_ci(   Y_L = Y_cal,   f_L = Yhat_cal,   f_U = Yhat_test,   alpha = ALPHA )  # 4. Rogan-Gladen rg_result <- rg_point_and_ci(   p_hat = p_hat,   q0_hat = q0_hat,   q1_hat = q1_hat,   n = n,   m0 = m0,   m1 = m1,   alpha = ALPHA )  # 5. EIF (Efficient Influence Function) eif_result <- eif_point_and_ci(   Y_cal = Y_cal,   Yhat_cal = Yhat_cal,   Yhat_test = Yhat_test,   alpha = ALPHA )  # 6. MLE (Joint Maximum Likelihood) mle_result <- mle_point_and_ci(   Y_cal = Y_cal,   Yhat_cal = Yhat_cal,   Yhat_test = Yhat_test,   alpha = ALPHA )"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/real-data-example.html","id":"results-summary","dir":"Articles","previous_headings":"","what":"Results Summary","title":"Real Data Example: GPT-4o-mini as Judge","text":"Estimator Comparison: Claude Opus 4 Win Rate","code":"results <- tibble(   Method = c(\"True\", \"Naive\", \"PPI\", \"PPI++\", \"Rogan-Gladen\", \"EIF\", \"MLE\"),   Estimate = c(     theta_true,     naive_theta,     ppi_result$theta,     ppi_pp_result$theta,     rg_result$theta,     eif_result$theta,     mle_result$theta   ),   CI_Lower = c(NA, naive_ci[1], ppi_result$ci_lower, ppi_pp_result$ci_lower,                rg_result$ci_lower, eif_result$ci_lower, mle_result$ci_lower),   CI_Upper = c(NA, naive_ci[2], ppi_result$ci_upper, ppi_pp_result$ci_upper,                rg_result$ci_upper, eif_result$ci_upper, mle_result$ci_upper) ) %>%   mutate(     Bias = round(Estimate - theta_true, 4),     CI_Width = round(CI_Upper - CI_Lower, 3),     Covers = CI_Lower <= theta_true & CI_Upper >= theta_true   )  knitr::kable(results, digits = 3, caption = \"Estimator Comparison: Claude Opus 4 Win Rate\")"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/real-data-example.html","id":"visualization","dir":"Articles","previous_headings":"","what":"Visualization","title":"Real Data Example: GPT-4o-mini as Judge","text":"","code":"METHOD_COLORS <- c(   \"Naive\" = \"#525252\",   \"PPI\" = \"#1B9E77\",   \"PPI++\" = \"#D95F02\",   \"Rogan-Gladen\" = \"#7570B3\",   \"EIF\" = \"#E6AB02\",   \"MLE\" = \"#E7298A\" )  plot_df <- results %>%   filter(Method != \"True\") %>%   mutate(Method = factor(Method, levels = names(METHOD_COLORS)))  ggplot(plot_df, aes(x = Method, y = Estimate, color = Method)) +   geom_point(size = 4) +   geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper),                 width = 0.2, linewidth = 1.2) +   geom_hline(yintercept = theta_true, linetype = \"solid\",              color = \"black\", linewidth = 1) +   annotate(\"text\", x = 0.55, y = theta_true + 0.025,            label = paste(\"True =\", round(theta_true, 3)),            hjust = 0, fontface = \"bold\") +   scale_color_manual(values = METHOD_COLORS) +   labs(     title = \"Claude Opus 4 vs Gemini 2.5 Pro Win Rate\",     subtitle = paste0(\"Judge: GPT-4o-mini | \", round(100*m/N),                       \"% labeled | \", 100*(1-ALPHA), \"% CI\"),     x = NULL,     y = \"Estimated Win Rate (Claude)\"   ) +   theme_bw(base_size = 12) +   theme(     legend.position = \"none\",     plot.title = element_text(hjust = 0.5, face = \"bold\"),     plot.subtitle = element_text(hjust = 0.5),     axis.text.x = element_text(angle = 30, hjust = 1)   ) +   coord_cartesian(ylim = c(0, 1))"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/real-data-example.html","id":"interpretation","dir":"Articles","previous_headings":"","what":"Interpretation","title":"Real Data Example: GPT-4o-mini as Judge","text":"Naive estimate: Uses judge predictions without correction. May biased GPT-4o-mini systematic preferences. PPI/PPI++: Uses calibration residuals correct systematic differences judge human preferences. Rogan-Gladen: Classical misclassification correction using estimated sensitivity/specificity. EIF: Efficient influence function estimator achieves semiparametric efficiency. MLE: Joint maximum likelihood estimation simultaneously estimates prevalence, sensitivity, specificity. debiased estimates closer true human preference rate, confidence intervals provide valid coverage even judge imperfect.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/articles/real-data-example.html","id":"session-info","dir":"Articles","previous_headings":"","what":"Session Info","title":"Real Data Example: GPT-4o-mini as Judge","text":"","code":"sessionInfo() #> R version 4.4.1 (2024-06-14) #> Platform: aarch64-apple-darwin20 #> Running under: macOS 15.7 #>  #> Matrix products: default #> BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib  #> LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0 #>  #> locale: #> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 #>  #> time zone: America/Los_Angeles #> tzcode source: internal #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] ggplot2_4.0.1            dplyr_1.1.4              jsonlite_2.0.0           #> [4] debiasLLMReporting_0.0.1 #>  #> loaded via a namespace (and not attached): #>  [1] gtable_0.3.6        highr_0.11          compiler_4.4.1      #>  [4] tidyselect_1.2.1    jquerylib_0.1.4     systemfonts_1.1.0   #>  [7] scales_1.4.0        textshaping_0.4.0   yaml_2.3.10         #> [10] fastmap_1.2.0       R6_2.6.1            labeling_0.4.3      #> [13] generics_0.1.4      knitr_1.48          htmlwidgets_1.6.4   #> [16] tibble_3.3.0        desc_1.4.3          bslib_0.9.0         #> [19] pillar_1.11.1       RColorBrewer_1.1-3  rlang_1.1.6         #> [22] cachem_1.1.0        xfun_0.48           fs_1.6.6            #> [25] sass_0.4.10         S7_0.2.1            cli_3.6.5           #> [28] pkgdown_2.1.1       withr_3.0.2         magrittr_2.0.4      #> [31] digest_0.6.37       grid_4.4.1          rstudioapi_0.17.0   #> [34] lifecycle_1.0.4     vctrs_0.6.5         evaluate_1.0.1      #> [37] glue_1.8.0          farver_2.1.2        numDeriv_2016.8-1.1 #> [40] ragg_1.3.3          rmarkdown_2.28      tools_4.4.1         #> [43] pkgconfig_2.0.3     htmltools_0.5.8.1"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Yiqun Chen. Author, maintainer. Moran Guo. Author.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Chen Y, Guo M (2026). debiasLLMReporting: Debiased Inference LLM---Judge Evaluations. R package version 0.0.1, https://yiqunchen.github.io/debias-llm---judge.","code":"@Manual{,   title = {debiasLLMReporting: Debiased Inference for LLM-as-a-Judge Evaluations},   author = {Yiqun Chen and Moran Guo},   year = {2026},   note = {R package version 0.0.1},   url = {https://yiqunchen.github.io/debias-llm-as-a-judge}, }"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/index.html","id":"debiasllmreporting","dir":"","previous_headings":"","what":"debiasLLMReporting","title":"Debiased Inference for LLM-as-a-Judge Evaluations","text":"R package comparing methods correct measurement error using LLM---judge (ML/AI/… classifiers) classifiers estimate population prevalence.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/index.html","id":"overview","dir":"","previous_headings":"debiasLLMReporting","what":"Overview","title":"Debiased Inference for LLM-as-a-Judge Evaluations","text":"using large language models (LLMs) classifiers estimate prevalence outcome population, LLM’s imperfect accuracy introduces bias. package provides simulation tools estimators study correct measurement error. compare five main approaches: Measurement Error Correction: Classical approach using Rogan-Gladen estimator, corrects misclassification using estimated sensitivity (q1) specificity (q0) calibration sample. Prediction-Powered Inference (PPI): Uses small labeled dataset debias predictions larger unlabeled dataset, combining human labels LLM predictions. PPI++: extension PPI optimizes tuning parameter (lambda) minimize variance maintaining valid confidence intervals. EIF: estimator based semi-parametric efficiency theory, coincides PPI++ optimal tuning parameter binary case. MLE: Joint maximum likelihood estimation misclassification model, simultaneously estimating prevalence, sensitivity, specificity.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/index.html","id":"installation","dir":"","previous_headings":"debiasLLMReporting","what":"Installation","title":"Debiased Inference for LLM-as-a-Judge Evaluations","text":"","code":"# Install from GitHub # install.packages(\"remotes\")  # if needed remotes::install_github(\"yiqunchen/debias-llm-as-a-judge\")"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/index.html","id":"tutorials-and-use","dir":"","previous_headings":"debiasLLMReporting","what":"Tutorials and Use","title":"Debiased Inference for LLM-as-a-Judge Evaluations","text":"Visit https://yiqunchen.github.io/debias-llm---judge/ tutorials examples. Please file issue request tutorial currently included.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/index.html","id":"citation","dir":"","previous_headings":"debiasLLMReporting","what":"Citation","title":"Debiased Inference for LLM-as-a-Judge Evaluations","text":"use debias-llm---judge analysis, please cite manuscript: TBA","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/index.html","id":"quick-start","dir":"","previous_headings":"debiasLLMReporting","what":"Quick Start","title":"Debiased Inference for LLM-as-a-Judge Evaluations","text":"","code":"library(debiasLLMReporting)  # Simulated example set.seed(2026) N <- 500; m <- 50 theta_true <- 0.6; q0 <- 0.85; q1 <- 0.80  # Generate data Y_all <- rbinom(N, 1, theta_true) Yhat_all <- ifelse(Y_all == 1, rbinom(N, 1, q1), 1 - rbinom(N, 1, q0))  # Split into calibration and test idx_cal <- sample(N, m) Y_cal <- Y_all[idx_cal] Yhat_cal <- Yhat_all[idx_cal] Yhat_test <- Yhat_all[-idx_cal]  # Apply PPI++ result_ppi <- ppi_pp_point_and_ci(   Y_L = Y_cal, f_L = Yhat_cal, f_U = Yhat_test, alpha = 0.10 ) cat(\"PPI++ Estimate:\", round(result_ppi$theta, 3),     \"90% CI: [\", round(result_ppi$ci_lower, 3), \",\", round(result_ppi$ci_upper, 3), \"]\\n\")  # Apply EIF result_eif <- eif_point_and_ci(   Y_cal = Y_cal, Yhat_cal = Yhat_cal, Yhat_test = Yhat_test, alpha = 0.10 ) cat(\"EIF Estimate:\", round(result_eif$theta, 3),     \"90% CI: [\", round(result_eif$ci_lower, 3), \",\", round(result_eif$ci_upper, 3), \"]\\n\")  # Apply MLE result_mle <- mle_point_and_ci(   Y_cal = Y_cal, Yhat_cal = Yhat_cal, Yhat_test = Yhat_test, alpha = 0.10 ) cat(\"MLE Estimate:\", round(result_mle$theta, 3),     \"90% CI: [\", round(result_mle$ci_lower, 3), \",\", round(result_mle$ci_upper, 3), \"]\\n\")"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/index.html","id":"package-functions","dir":"","previous_headings":"debiasLLMReporting","what":"Package Functions","title":"Debiased Inference for LLM-as-a-Judge Evaluations","text":"Core estimators: rg_point_and_ci() - Rogan-Gladen estimator delta method CI ppi_point_and_ci() - Standard PPI estimator ppi_pp_point_and_ci() - PPI++ optimized lambda eif_point_and_ci() - Efficient influence function estimator mle_point_and_ci() - Joint MLE misclassification model","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/index.html","id":"reproducibility","dir":"","previous_headings":"debiasLLMReporting","what":"Reproducibility","title":"Debiased Inference for LLM-as-a-Judge Evaluations","text":"reproduce/ directory contains scripts replicate analyses: simulation_llm_vs_ppi.R - Monte Carlo simulation comparing estimators across parameter grid apply_estimators.R - Apply estimators real LLM judge evaluation data Results saved timestamped directories results/.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/index.html","id":"license","dir":"","previous_headings":"debiasLLMReporting","what":"License","title":"Debiased Inference for LLM-as-a-Judge Evaluations","text":"MIT","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":null,"dir":"","previous_headings":"","what":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"compare two estimators scalar parameter (= [Z] = (Z=1)): Rogan–Gladen (RG) misclassification‐corrected estimator Vanilla Prediction–Powered Inference (PPI) show fit general “PPI transform (g)” framework.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"id_1-setup-and-notation-binary-misclassification","dir":"","previous_headings":"","what":"1. Setup and notation (binary misclassification)","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"True (human) label: (Z {0,1}) Target parameter: (= [Z] = (Z=1)) Surrogate / judge label: (Z {0,1}) Define: Sensitivity: [ q_1 = (Z = 1 Z = 1) ] Specificity: [ q_0 = (Z = 0 Z = 0) ] Youden’s (J): [ J = q_0 + q_1 - 1 (0,1]  ]","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"data","dir":"","previous_headings":"1. Setup and notation (binary misclassification)","what":"Data","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"Big test set size (n): (Z_i), (=1,,n) [ p =  _{=1}^n Z_i p := (Z = 1). ] Small calibration set size (m): (Z_j,Z_j), (j=1,,m). Goal: estimate (= [Z]).","code":""},{"path":[]},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"id_21-rogangladen-rg-estimator","dir":"","previous_headings":"2. Two estimators","what":"2.1 Rogan–Gladen (RG) estimator","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"standard misclassification model, [ p = (Z = 1) = q_1 + (1-q_0)(1-) = J,+ (1-q_0). ] Solve (): [ = . ] Plug empirical estimates: - (p = _{=1}^n Z_i) - (q_0 = ) - (q_1 = ) RG estimator : [ _{} = . ] pretend (q_0,q_1) known ((m) huge uncertainty negligible), just linear transform (p): [ _{} . ]","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"id_22-vanilla-ppi-estimator","dir":"","previous_headings":"2. Two estimators","what":"2.2 Vanilla PPI estimator","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"Let: - (Y := Z) - (f(X) := Z) (ignore (X) explicitly; treat (Z) function data) : [  = [Z] = [Z] - [Z - Z] = p - , ] [ := [Z - Z]. ] Estimators: test set: [ p =  _{=1}^n Z_i. ] calibration set: [  =  _{j=1}^m (Z_j - Z_j). ] PPI estimator: [ _{} = p - . ] uses average prediction error calibration set; never explicitly uses (q_0,q_1).","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"id_3-asymptotic-variances-simple-regime","dir":"","previous_headings":"","what":"3. Asymptotic variances (simple regime)","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"Assumptions comparison: Test calibration sets ..d. population. (n) (test size) (m) (calibration size) large. Treat ((q_0,q_1)), hence (J), fixed constants (ignoring estimation variance now).","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"id_31-variance-of-rg-estimator-known-q_0q_1","dir":"","previous_headings":"3. Asymptotic variances (simple regime)","what":"3.1 Variance of RG estimator (known (q_0,q_1))","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"(q_0,q_1) known, [ _{} =  ] linear (p). Since (p) binomial proportion: [ (p) , ] obtain: [ (_{}) .  ] usual binomial variance scaled (1/J^2).","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"id_32-variance-of-ppi-estimator","dir":"","previous_headings":"3. Asymptotic variances (simple regime)","what":"3.2 Variance of PPI estimator","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"General result mean PPI (Angelopoulos et al.): () constructed : - large “unlabeled” set size (n_{}) (f(X)), - “labeled” set size (n_{}) (f(X)) (Y), asymptotically: [ (_{})   + . ] case: - (f(X) = Z), - (Y = Z), - (n_{}=n) (test set), - (n_{}=m) (calibration set), : [ (_{})   + .  ] : [ (Z) = p(1-p). ] Also, (Z - Z {-1,0,1}): (Z - Z = +1) false positives (FP), (Z - Z = -1) false negatives (FN), (Z - Z = 0) otherwise. Let: - (() = (Z = 1, Z = 0)), - (() = (Z = 0, Z = 1)). Let (= [Z - Z] = p - ). : [ (Z - Z) = () + () - ^2. ] Therefore: [ (_{})   + . ] (m) large enough calibration term negligible: [ (_{}) . ]","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"id_33-direct-comparison-when-calibration-is-large","dir":"","previous_headings":"3. Asymptotic variances (simple regime)","what":"3.3 Direct comparison when calibration is large","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"“large calibration” regime (ignore calibration term (PPI)): PPI variance: [ (_{}) . ] RG variance: [ (_{}) . ] Since (0 < J ), (1/J^2 ), strict inequality whenever judge imperfect (.e., (J < 1)). Thus: [ ({})  ({}), ] equality judge perfect ((J = 1)). finite (m), PPI gains additional term ((Z - Z)/m), RG gains extra variance estimating (q_0,q_1) (written ), uniform dominance parameter regimes. structural difference remains: RG amplifies test‑set noise (1/J^2), PPI keeps test‑set noise base (p(1-p)/n) adds separate calibration term.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"id_4-ppi-with-a-general-transform-g","dir":"","previous_headings":"","what":"4. PPI with a general transform (g)","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"Let (S) denote surrogate ((S = Z)), let (X) additional features (possibly ignored). function (g(S,X)), define PPI‑style estimator: [ g = {} - _{}. ] : [ [_g] = [g(S,X)] - [g(S,X) - Z] = [Z] = . ] Asymptotic variance: [ (_g)   + .  ] Choosing (g) , therefore, variance‑optimization problem.","code":""},{"path":[]},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"a-identity--vanilla-ppi","dir":"","previous_headings":"4. PPI with a general transform (g) > 4.1 Special choices of (g)","what":"(a) Identity / vanilla PPI","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"[ g_{}(S) = S = Z. ] : [ (_{})   + . ]","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"b-rogangladen-transform-as-g","dir":"","previous_headings":"4. PPI with a general transform (g) > 4.1 Special choices of (g)","what":"(b) Rogan–Gladen transform as (g)","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"Suppose fixed pilot estimates (q_0,q_1) ((q_0,q_1)), define (J = q_0 + q_1 - 1). Define: [ g_{}(S) = . ] : [  {} g{}(S_i) = , ] RG plug‑estimator (q_0,q_1) place (q_0,q_1). stop (omit rectifier), recover usual RG‑style estimator (fixed (q_0,q_1)). full PPI: [ {g{}} =  {} g{}(S_i) -  {} ( g{}(S_j) - Z_j ), ] still unbiased () uses calibration set correct residual bias (g_{}).","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"c-learned--optimal-g","dir":"","previous_headings":"4. PPI with a general transform (g) > 4.1 Special choices of (g)","what":"(c) Learned / optimal (g)","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"generally, can learn (g) calibration set (e.g., via regression) (g(S,X)) approximates (Z) well possible, thereby minimizing variance ((*)). RePPI shows construct asymptotically optimal (g) via influence‑function regression.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"id_5-summary","dir":"","previous_headings":"","what":"5. Summary","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"RG estimator: [ _{} = . ] Vanilla PPI estimator ((g(S)=S)): [ {} = p -  =  {} Z_i _{} (Z_j - Z_j). ] General PPI transform (g): [ g =  {} g(S_i,X_i) _{} (g(S_j,X_j) - Z_j). ] large‑calibration limit judge imperfect ((J<1)), PPI asymptotic variance (p(1-p)/n), whereas RG variance (p(1-p)/(nJ^2)); thus, PPI asymptotically efficient regime.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"multiclass-k-class-misclassification-rg-vs-ppi","dir":"","previous_headings":"","what":"Multiclass (K-Class) Misclassification: RG vs PPI","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"now consider K-class categorical outcome, observed noisy surrogate (e.g., LLM labeler classifier). compare: multiclass Rogan–Gladen / confusion-matrix inversion estimator, Prediction-Powered Inference (PPI) estimator, show fit general PPI--transform-(g) framework.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"id_1-setup-and-notation-k-class-categorical-misclassification","dir":"","previous_headings":"","what":"1. Setup and Notation (K-Class Categorical Misclassification)","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"Let true label (Y {1,,K}) target class probabilities (= (_1,,K)^). surrogate (S{1,,K}) obeys misclassification matrix (M) entries (M{ab} = (S=Y=b)) observed surrogate probabilities (p = (p_1,,p_K)^) satisfying (p = M) (assume (M) invertible). Data: Large test set size (n): observe (S_i) ; () empirical class distribution (S). Calibration set size (m): observe ((Y_j,S_j)); estimate (M) via (M_{ab} = #{j:S_j=,Y_j=b} / #{j:Y_j=b}). Goal: estimate ().","code":""},{"path":[]},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"id_21-multiclass-rogangladen--confusion-matrix-inversion","dir":"","previous_headings":"2. Two Estimators","what":"2.1 Multiclass Rogan–Gladen / Confusion-Matrix Inversion","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"Since (p = M), true prevalence vector (= M^{-1}p). Plugging (M) () yields: [ {} = M^{-1} . ] Componentwise ({,k}) (k)-th entry (M^{-1}). generalizes binary RG inverting estimated confusion matrix.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"id_22-ppi-estimator-identity-g-class-by-class","dir":"","previous_headings":"2. Two Estimators","what":"2.2 PPI Estimator (Identity (g), Class-by-Class)","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"Represent (Y) via one-hot vectors (Y = (1{Y=1},,1{Y=K})^), (= [Y]). class (k), set (g{}k(S) = 1{S=k}) apply PPI: [ {}k = {=1}n g{}k(S_i) + {j=1}m (Y_j^{(k)} - g^{}k(S_j)). ] Stacking gives: [ {} = {=1}n g^{}(S_i) + _{j=1}^m (Y_j - g^{}(S_j)). ]","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"id_3-asymptotic-variances-sketch","dir":"","previous_headings":"","what":"3. Asymptotic Variances (Sketch)","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"Treat estimators vectors (^K).","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"id_31-rg-estimator-known-m","dir":"","previous_headings":"3. Asymptotic Variances (Sketch)","what":"3.1 RG Estimator (Known (M))","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"(M) known, ({} = M^{-1}) multinomial covariance: [ ({}) , M^{-1}((p) - pp)(M{-1})^. ] Estimating (M) adds delta-method variance term.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"id_32-ppi-estimator-vector-form","dir":"","previous_headings":"3. Asymptotic Variances (Sketch)","what":"3.2 PPI Estimator (Vector Form)","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"vector-valued (g(S)): [ g = {=1}^n g(S_i) + _{j=1}^m (Y_j - g(S_j)), ] [ (_g) ,(g(S)) + ,(Y - g(S)). ] Setting (g = g^{}) produces classwise PPI variances. (g) choices target lower variance.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"id_4-ppi-with-transform-g-multiclass","dir":"","previous_headings":"","what":"4. PPI with Transform (g) (Multiclass)","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"Let (g(S,X)^K). [ g = {=1}^n g(S_i,X_i) + _{j=1}^m (Y_j - g(S_j,X_j)) ] remains unbiased covariance ((g) + (Y-g)). Examples: Identity / naïve PPI: (g^{}(S)) (one-hot). Confusion-matrix transform (RG): (W=M^{-1}), set (g^{}(S)=W e_S) (column (S) (W)). plug-term alone recover RG; adding rectifier keeps unbiasedness reducing variance. Soft/learned (g): fit (g_k(S,X)(Y=kS,X)), e.g., via multinomial logistic regression, plug formula. shows RG special linear (g), PPI can incorporate RG learned transforms trade bias variance flexibly.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"id_42-projection-onto-the-simplex","dir":"","previous_headings":"4. PPI with Transform (g) (Multiclass)","what":"4.2 Projection onto the simplex","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"Rather ad-hoc clipping coordinate, can enforce probability-simplex constraints multiclass estimator projecting: [  = {{K-1}} |- |2^2,  {K-1} = {: k , {k=1}^K _k = 1}. ] minimizer closed-form “sorting + threshold” solution identical sparsemax/simplex-projection algorithm. returns closest valid probability vector unconstrained estimate standard quantification survey calibration workflows.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/methods-setup.html","id":"id_43-constrained-rg--constrained-likelihood","dir":"","previous_headings":"4. PPI with Transform (g) (Multiclass)","what":"4.3 Constrained RG / constrained likelihood","title":"PPI vs Rogan–Gladen (RG) in Binary Misclassification","text":"RG side, instead computing (M^{-1} ) directly, can solve constrained least-squares problem [ {{K-1}} | - M |_2^2, ] constrained multinomial likelihood. enforces non-negativity sum--one constraint “inside” estimation, can viewed RG simplex constraint baked parameter level. projection-based RG constrained least-squares RG coincide (M) invertible solution lies simplex interior, differ inversion leave simplex. implemented companion code can compare unconstrained, projected, fully constrained variants.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/build_line_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Generic line plot builder — build_line_plot","title":"Generic line plot builder — build_line_plot","text":"Generic line plot builder","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/build_line_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generic line plot builder — build_line_plot","text":"","code":"build_line_plot(   data,   x_var,   y_var,   color_var,   linetype_var = NULL,   facet_formula,   title,   subtitle = \"\",   y_lab,   color_lab = \"Method\",   linetype_lab = \"neg:pos\",   hline = NULL,   hline_style = \"dotted\",   x_breaks = NULL,   y_limits = NULL,   use_aggregate_theme = FALSE,   point_size = 1.5,   line_width = 1 )"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/build_line_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generic line plot builder — build_line_plot","text":"data Data frame x_var X variable (unquoted) y_var Y variable (unquoted) color_var Color variable (unquoted) linetype_var Optional linetype variable (unquoted), NULL none facet_formula Facet formula (e.g., q0_lab ~ q1_lab) title Plot title subtitle Plot subtitle y_lab Y-axis label color_lab Color legend label linetype_lab Linetype legend label (applicable) hline Optional horizontal line y-intercept hline_style \"dashed\" \"dotted\" x_breaks X-axis breaks y_limits Optional y-axis limits (c(min, max)) use_aggregate_theme Use larger theme aggregate plots point_size Point size line_width Line width","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/build_line_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generic line plot builder — build_line_plot","text":"ggplot object","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/eif_continuous_point_and_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"EIF estimator for continuous surrogates with calibration — eif_continuous_point_and_ci","title":"EIF estimator for continuous surrogates with calibration — eif_continuous_point_and_ci","text":"EIF estimator continuous surrogates calibration","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/eif_continuous_point_and_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"EIF estimator for continuous surrogates with calibration — eif_continuous_point_and_ci","text":"","code":"eif_continuous_point_and_ci(   Y_cal,   Yhat_cal,   Yhat_test,   calibration_method = \"linear\",   alpha = 0.1 )"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/eif_discrete_point_and_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"EIF estimator with per-category calibration (optimal for discrete) — eif_discrete_point_and_ci","title":"EIF estimator with per-category calibration (optimal for discrete) — eif_discrete_point_and_ci","text":"Learns g: 1,...,K -> R g(z) = EY | Yhat = z discrete Yhat, just conditional mean per category","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/eif_discrete_point_and_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"EIF estimator with per-category calibration (optimal for discrete) — eif_discrete_point_and_ci","text":"","code":"eif_discrete_point_and_ci(Y_cal, Yhat_cal, Yhat_test, alpha = 0.1)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/eif_gam_discrete_point_and_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"EIF estimator with GAM calibration — eif_gam_discrete_point_and_ci","title":"EIF estimator with GAM calibration — eif_gam_discrete_point_and_ci","text":"Fits g(Yhat) using GAM smooth term discrete Yhat categories, behaves similarly per-category","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/eif_gam_discrete_point_and_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"EIF estimator with GAM calibration — eif_gam_discrete_point_and_ci","text":"","code":"eif_gam_discrete_point_and_ci(Y_cal, Yhat_cal, Yhat_test, alpha = 0.1)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/eif_linear_discrete_point_and_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"EIF estimator with linear calibration (suboptimal for discrete) — eif_linear_discrete_point_and_ci","title":"EIF estimator with linear calibration (suboptimal for discrete) — eif_linear_discrete_point_and_ci","text":"Fits g(Yhat) = + b*Yhat using linear regression suboptimal Yhat discrete shows importance proper calibration","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/eif_linear_discrete_point_and_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"EIF estimator with linear calibration (suboptimal for discrete) — eif_linear_discrete_point_and_ci","text":"","code":"eif_linear_discrete_point_and_ci(Y_cal, Yhat_cal, Yhat_test, alpha = 0.1)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/eif_point_and_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"EIF (Efficient Influence Function) estimator for binary surrogates. — eif_point_and_ci","title":"EIF (Efficient Influence Function) estimator for binary surrogates. — eif_point_and_ci","text":"Implements post-stratified efficient estimator EIF theory. binary surrogates, estimates \\(\\theta = E[Y]\\) using: $$\\hat\\theta_{EIF} = (1-\\hat p)\\hat\\mu_0 + \\hat p \\hat\\mu_1$$ \\(\\hat\\mu_0, \\hat\\mu_1\\) conditional means Y given \\(\\hat Y = 0, 1\\) estimated calibration data, \\(\\hat p\\) proportion \\(\\hat Y = 1\\) test sample.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/eif_point_and_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"EIF (Efficient Influence Function) estimator for binary surrogates. — eif_point_and_ci","text":"","code":"eif_point_and_ci(Y_cal, Yhat_cal, Yhat_test, alpha = 0.1)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/eif_point_and_ci.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"EIF (Efficient Influence Function) estimator for binary surrogates. — eif_point_and_ci","text":"Y_cal Vector true labels calibration set. Yhat_cal Vector surrogate predictions (0/1) calibration set. Yhat_test Vector surrogate predictions (0/1) test set. alpha Miscoverage level confidence interval.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/eif_point_and_ci.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"EIF (Efficient Influence Function) estimator for binary surrogates. — eif_point_and_ci","text":"list containing: theta Point estimate prevalence. var Estimated variance estimator. ci_lower Lower bound confidence interval. ci_upper Upper bound confidence interval. mu0_hat Estimated \\(E[Y | \\hat Y = 0]\\). mu1_hat Estimated \\(E[Y | \\hat Y = 1]\\). p_hat Proportion \\(\\hat Y = 1\\) test set.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/eif_point_and_ci.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"EIF (Efficient Influence Function) estimator for binary surrogates. — eif_point_and_ci","text":"estimator asymptotically efficient Model (surrogates conditionally independent outcomes given covariates).","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/eif_spline_discrete_point_and_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"EIF estimator with spline calibration — eif_spline_discrete_point_and_ci","title":"EIF estimator with spline calibration — eif_spline_discrete_point_and_ci","text":"Fits g(Yhat) using natural splines discrete Yhat categories, behaves similarly per-category","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/eif_spline_discrete_point_and_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"EIF estimator with spline calibration — eif_spline_discrete_point_and_ci","text":"","code":"eif_spline_discrete_point_and_ci(Y_cal, Yhat_cal, Yhat_test, alpha = 0.1)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/estimate_confusion_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate a multiclass confusion matrix using calibration data. — estimate_confusion_matrix","title":"Estimate a multiclass confusion matrix using calibration data. — estimate_confusion_matrix","text":"Estimate multiclass confusion matrix using calibration data.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/estimate_confusion_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate a multiclass confusion matrix using calibration data. — estimate_confusion_matrix","text":"","code":"estimate_confusion_matrix(S, Y, K, laplace = 0.001)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/estimate_confusion_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate a multiclass confusion matrix using calibration data. — estimate_confusion_matrix","text":"S Observed surrogate labels calibration set. Y True labels calibration set. K Number classes. laplace Small positive smoothing constant added cell.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/estimate_confusion_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate a multiclass confusion matrix using calibration data. — estimate_confusion_matrix","text":"K x K column-stochastic matrix \\(\\hat{M}\\) entries \\(P(S=\\mid Y=b)\\).","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/fit_calibration_gam.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit calibration model using GAM — fit_calibration_gam","title":"Fit calibration model using GAM — fit_calibration_gam","text":"Fit calibration model using GAM","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/fit_calibration_gam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit calibration model using GAM — fit_calibration_gam","text":"","code":"fit_calibration_gam(Y_cal, Yhat_cal, k = 10)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/fit_calibration_isotonic.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit calibration model using isotonic regression — fit_calibration_isotonic","title":"Fit calibration model using isotonic regression — fit_calibration_isotonic","text":"Fit calibration model using isotonic regression","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/fit_calibration_isotonic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit calibration model using isotonic regression — fit_calibration_isotonic","text":"","code":"fit_calibration_isotonic(Y_cal, Yhat_cal)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/fit_calibration_linear.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit calibration model using linear regression — fit_calibration_linear","title":"Fit calibration model using linear regression — fit_calibration_linear","text":"Fit calibration model using linear regression","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/fit_calibration_linear.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit calibration model using linear regression — fit_calibration_linear","text":"","code":"fit_calibration_linear(Y_cal, Yhat_cal)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/fit_calibration_spline.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit calibration model using cubic spline — fit_calibration_spline","title":"Fit calibration model using cubic spline — fit_calibration_spline","text":"Fit calibration model using cubic spline","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/fit_calibration_spline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit calibration model using cubic spline — fit_calibration_spline","text":"","code":"fit_calibration_spline(Y_cal, Yhat_cal, df = 4)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/fit_misclass_mle.html","id":null,"dir":"Reference","previous_headings":"","what":"Joint MLE under the binary misclassification model. — fit_misclass_mle","title":"Joint MLE under the binary misclassification model. — fit_misclass_mle","text":"Estimates \\theta, q0, q1 maximizing joint likelihood built labeled calibration data unlabeled surrogate outputs, obtains standard errors via inverse Fisher information (observed expected).","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/fit_misclass_mle.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Joint MLE under the binary misclassification model. — fit_misclass_mle","text":"","code":"fit_misclass_mle(y_cal, yhat_cal, yhat_test, level = 0.9)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/fit_misclass_mle.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Joint MLE under the binary misclassification model. — fit_misclass_mle","text":"y_cal Human labels calibration split (0/1). yhat_cal Surrogate predictions calibration split (0/1). yhat_test Surrogate predictions unlabeled/test split (0/1). level Confidence level Wald intervals. Defaults 0.90.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/fit_misclass_mle.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Joint MLE under the binary misclassification model. — fit_misclass_mle","text":"list containing MLEs, variance-covariance matrices observed expected Fisher information, Wald intervals \\theta.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/fit_misclassification_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit covariate-dependent misclassification models. — fit_misclassification_models","title":"Fit covariate-dependent misclassification models. — fit_misclassification_models","text":"Estimates \\(P(S=1 \\mid Y=1, X)\\) \\(P(S=1 \\mid Y=0, X)\\) via logistic regression calibration data.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/fit_misclassification_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit covariate-dependent misclassification models. — fit_misclassification_models","text":"","code":"fit_misclassification_models(covariates, Y, S)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/fit_misclassification_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit covariate-dependent misclassification models. — fit_misclassification_models","text":"covariates Data frame covariates observed calibration set. Y Calibration human labels. S Calibration surrogate outputs (0/1).","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/fit_misclassification_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit covariate-dependent misclassification models. — fit_misclassification_models","text":"list two glm objects: one positive class (tp_model) one negative class (fp_model).","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/fit_reppi_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a RePPI-style logistic regression for \\(g(S,X)\\). — fit_reppi_model","title":"Fit a RePPI-style logistic regression for \\(g(S,X)\\). — fit_reppi_model","text":"Fit RePPI-style logistic regression \\(g(S,X)\\).","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/fit_reppi_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a RePPI-style logistic regression for \\(g(S,X)\\). — fit_reppi_model","text":"","code":"fit_reppi_model(covariates, S, Y)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/fit_reppi_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a RePPI-style logistic regression for \\(g(S,X)\\). — fit_reppi_model","text":"covariates Data frame covariates calibration set. S Calibration surrogate outputs (0/1). Y Calibration human labels (0/1).","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/fit_reppi_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a RePPI-style logistic regression for \\(g(S,X)\\). — fit_reppi_model","text":"Fitted glm predicting Y S covariates (interactions).","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/format_label_ratio.html","id":null,"dir":"Reference","previous_headings":"","what":"Format a labeling ratio as a percentage string — format_label_ratio","title":"Format a labeling ratio as a percentage string — format_label_ratio","text":"Format labeling ratio percentage string","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/format_label_ratio.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Format a labeling ratio as a percentage string — format_label_ratio","text":"","code":"format_label_ratio(x)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/format_label_ratio.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Format a labeling ratio as a percentage string — format_label_ratio","text":"x Numeric vector label fractions (0 1).","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/format_label_ratio.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Format a labeling ratio as a percentage string — format_label_ratio","text":"character vector like \"5% labeled\".","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_confusion_inverse.html","id":null,"dir":"Reference","previous_headings":"","what":"Confusion-matrix inverse g-transform (multiclass RG). — g_confusion_inverse","title":"Confusion-matrix inverse g-transform (multiclass RG). — g_confusion_inverse","text":"Confusion-matrix inverse g-transform (multiclass RG).","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_confusion_inverse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Confusion-matrix inverse g-transform (multiclass RG). — g_confusion_inverse","text":"","code":"g_confusion_inverse(S, context)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_confusion_inverse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Confusion-matrix inverse g-transform (multiclass RG). — g_confusion_inverse","text":"S Integer vector surrogate labels. context List containing inverse confusion matrix W_inv.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_confusion_inverse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Confusion-matrix inverse g-transform (multiclass RG). — g_confusion_inverse","text":"Matrix whose rows W_inv %*% e_S.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_covariate_rg.html","id":null,"dir":"Reference","previous_headings":"","what":"Covariate-adjusted Rogan–Gladen transform. — g_covariate_rg","title":"Covariate-adjusted Rogan–Gladen transform. — g_covariate_rg","text":"Uses covariate-specific estimates \\(q_0(X)\\) \\(q_1(X)\\) form adjusted surrogate via RG formula.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_covariate_rg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Covariate-adjusted Rogan–Gladen transform. — g_covariate_rg","text":"","code":"g_covariate_rg(S, covariates, models, eps = 1e-04)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_covariate_rg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Covariate-adjusted Rogan–Gladen transform. — g_covariate_rg","text":"S Surrogate outputs (0/1) either labeled unlabeled data. covariates Data frame covariates matching S. models Output fit_misclassification_models. eps Small positive number avoid division zero.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_covariate_rg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Covariate-adjusted Rogan–Gladen transform. — g_covariate_rg","text":"Numeric vector adjusted surrogates.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_identity_multiclass.html","id":null,"dir":"Reference","previous_headings":"","what":"Identity g-transform for multiclass surrogates. — g_identity_multiclass","title":"Identity g-transform for multiclass surrogates. — g_identity_multiclass","text":"Identity g-transform multiclass surrogates.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_identity_multiclass.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Identity g-transform for multiclass surrogates. — g_identity_multiclass","text":"","code":"g_identity_multiclass(S, context)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_identity_multiclass.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Identity g-transform for multiclass surrogates. — g_identity_multiclass","text":"S Integer vector surrogate class labels \\(\\{1,...,K\\}\\). context List containing least K.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_identity_multiclass.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Identity g-transform for multiclass surrogates. — g_identity_multiclass","text":"Matrix one-hot rows representing S.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_posterior_prob.html","id":null,"dir":"Reference","previous_headings":"","what":"Posterior \\(P(Y = 1 \\mid \\hat Z = z)\\) under plug-in confusion matrix. — g_posterior_prob","title":"Posterior \\(P(Y = 1 \\mid \\hat Z = z)\\) under plug-in confusion matrix. — g_posterior_prob","text":"transform corresponds Bayes correction described general PPI--g framework: use pilot prevalence judge sensitivity/ specificity predict posterior probability underlying human label positive.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_posterior_prob.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Posterior \\(P(Y = 1 \\mid \\hat Z = z)\\) under plug-in confusion matrix. — g_posterior_prob","text":"","code":"g_posterior_prob(z, q0_pilot, q1_pilot, theta_pilot, eps = 1e-06)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_posterior_prob.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Posterior \\(P(Y = 1 \\mid \\hat Z = z)\\) under plug-in confusion matrix. — g_posterior_prob","text":"z Binary surrogate outputs (0/1) probabilities \\([0,1]\\). q0_pilot, q1_pilot Pilot specificity/sensitivity. theta_pilot Pilot prevalence (e.g., mean labeled outcomes). eps Small positive value preventing division zero.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_posterior_prob.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Posterior \\(P(Y = 1 \\mid \\hat Z = z)\\) under plug-in confusion matrix. — g_posterior_prob","text":"Posterior probabilities usable g-transform inside PPI.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_reppi_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict using the RePPI-style logistic regression. — g_reppi_predict","title":"Predict using the RePPI-style logistic regression. — g_reppi_predict","text":"Predict using RePPI-style logistic regression.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_reppi_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict using the RePPI-style logistic regression. — g_reppi_predict","text":"","code":"g_reppi_predict(S, covariates, model)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_reppi_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict using the RePPI-style logistic regression. — g_reppi_predict","text":"S Surrogate outputs (0/1). covariates Data frame covariates. model Fitted glm fit_reppi_model.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_reppi_predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict using the RePPI-style logistic regression. — g_reppi_predict","text":"Predicted probabilities, \\(P(Y = 1 \\mid S, X)\\).","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_rogan_gladen.html","id":null,"dir":"Reference","previous_headings":"","what":"Rogan–Gladen style transform usable inside PPI. — g_rogan_gladen","title":"Rogan–Gladen style transform usable inside PPI. — g_rogan_gladen","text":"Rogan–Gladen style transform usable inside PPI.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_rogan_gladen.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rogan–Gladen style transform usable inside PPI. — g_rogan_gladen","text":"","code":"g_rogan_gladen(z, q0_pilot, q1_pilot, eps = 0.001)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_rogan_gladen.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rogan–Gladen style transform usable inside PPI. — g_rogan_gladen","text":"z Numeric vector surrogate scores. q0_pilot, q1_pilot Pilot specificity sensitivity inputs. eps Tolerance avoid division zero judge random.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/g_rogan_gladen.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rogan–Gladen style transform usable inside PPI. — g_rogan_gladen","text":"Scores mapped onto corrected probability scale.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/generate_continuous_dgp.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate continuous outcome data with nonlinear X dependence — generate_continuous_dgp","title":"Generate continuous outcome data with nonlinear X dependence — generate_continuous_dgp","text":"True model: Y = f(X1, ..., X5) + noise X6-X10 noise features true model","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/generate_continuous_dgp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate continuous outcome data with nonlinear X dependence — generate_continuous_dgp","text":"","code":"generate_continuous_dgp(N, mu_x = 1, sigma_y = 2, seed = NULL)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/generate_continuous_dgp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate continuous outcome data with nonlinear X dependence — generate_continuous_dgp","text":"N Number observations mu_x Mean X features (controls distribution shift) sigma_y Noise standard deviation seed Random seed","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/generate_continuous_dgp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate continuous outcome data with nonlinear X dependence — generate_continuous_dgp","text":"List X matrix, Y vector, true conditional mean","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/generate_continuous_dgp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate continuous outcome data with nonlinear X dependence — generate_continuous_dgp","text":"KEY DESIGN: X ~ N(mu_x, 1). Train mu_x=1, test mu_x=1,3,5,7,9. creates distribution shift exposes bias misspecified models.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/generate_dgp_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Bernoulli responses with controllable prevalence. — generate_dgp_data","title":"Generate Bernoulli responses with controllable prevalence. — generate_dgp_data","text":"Draws ..d. binary outcomes specified prevalence. simpler exchangeable design appropriate estimators condition covariates.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/generate_dgp_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Bernoulli responses with controllable prevalence. — generate_dgp_data","text":"","code":"generate_dgp_data(N, theta, signal = 1)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/generate_dgp_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Bernoulli responses with controllable prevalence. — generate_dgp_data","text":"N Integer; number observations simulate. theta Target marginal prevalence Bernoulli outcome. signal Unused; retained API compatibility.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/generate_dgp_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Bernoulli responses with controllable prevalence. — generate_dgp_data","text":"list containing outcome vector Y.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/generate_discrete_dgp.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate discrete prediction data — generate_discrete_dgp","title":"Generate discrete prediction data — generate_discrete_dgp","text":"Z ~ Unif(1, ..., K), Y | Z ~ N(muZ, sigma^2), Yhat = Z","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/generate_discrete_dgp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate discrete prediction data — generate_discrete_dgp","text":"","code":"generate_discrete_dgp(N, mu = c(1, 5, 9), sigma = 1, seed = NULL)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/generate_discrete_dgp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate discrete prediction data — generate_discrete_dgp","text":"N Sample size mu Vector means mixture component sigma Standard deviation (components) seed Random seed","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/generate_discrete_dgp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate discrete prediction data — generate_discrete_dgp","text":"List Z, Y, Yhat","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/generate_plot_suite.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate all 5 standard plots for a given data subset — generate_plot_suite","title":"Generate all 5 standard plots for a given data subset — generate_plot_suite","text":"Generate 5 standard plots given data subset","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/generate_plot_suite.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate all 5 standard plots for a given data subset — generate_plot_suite","text":"","code":"generate_plot_suite(   ci_data,   q_data,   facet_formula,   filename_suffix,   subtitle = \"\",   save_fn,   x_breaks = NULL,   linetype_var = NULL,   use_aggregate = FALSE,   width = 14,   height = 12,   point_size = 1.5 )"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/generate_plot_suite.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate all 5 standard plots for a given data subset — generate_plot_suite","text":"ci_data CI summary data (filtered) q_data Q bias data (filtered) facet_formula Facet formula filename_suffix Suffix filenames subtitle Plot subtitle save_fn Function save plots (takes plot filename) x_breaks X-axis breaks linetype_var Optional linetype variable aggregate plots use_aggregate Use aggregate theme width Plot width height Plot height point_size Point size","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/generate_plot_suite.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate all 5 standard plots for a given data subset — generate_plot_suite","text":"NULL (saves plots side effect)","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/get_method_color_scale.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the fixed color scale for methods — get_method_color_scale","title":"Get the fixed color scale for methods — get_method_color_scale","text":"Get fixed color scale methods","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/get_method_color_scale.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the fixed color scale for methods — get_method_color_scale","text":"","code":"get_method_color_scale(methods = NULL)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/get_method_color_scale.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the fixed color scale for methods — get_method_color_scale","text":"methods Character vector method names include","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/get_method_color_scale.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the fixed color scale for methods — get_method_color_scale","text":"ggplot2 scale_color_manual","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/grab_theta.html","id":null,"dir":"Reference","previous_headings":"","what":"Grab the theta-specific summary from a joint MLE fit. — grab_theta","title":"Grab the theta-specific summary from a joint MLE fit. — grab_theta","text":"Grab theta-specific summary joint MLE fit.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/grab_theta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Grab the theta-specific summary from a joint MLE fit. — grab_theta","text":"","code":"grab_theta(fit, level = 0.9)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/grab_theta.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Grab the theta-specific summary from a joint MLE fit. — grab_theta","text":"fit Output fit_misclass_mle(). level Confidence level Wald interval.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/grab_theta.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Grab the theta-specific summary from a joint MLE fit. — grab_theta","text":"List theta_hat, observed/expected SEs, CIs.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/lambda_hat_ppi_pp_optimal.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute optimal lambda for PPI++ using closed-form solution. — lambda_hat_ppi_pp_optimal","title":"Compute optimal lambda for PPI++ using closed-form solution. — lambda_hat_ppi_pp_optimal","text":"optimal lambda minimizes asymptotic variance closed form: $$\\lambda^* = \\frac{\\text{Cov}(Y, f)}{\\text{Var}(f_L) + (m/n) \\text{Var}(f_U)}$$","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/lambda_hat_ppi_pp_optimal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute optimal lambda for PPI++ using closed-form solution. — lambda_hat_ppi_pp_optimal","text":"","code":"lambda_hat_ppi_pp_optimal(Y_L, f_L, f_U)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/lambda_hat_ppi_pp_optimal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute optimal lambda for PPI++ using closed-form solution. — lambda_hat_ppi_pp_optimal","text":"Y_L, f_L, f_U Observed labels surrogate scores.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/lambda_hat_ppi_pp_optimal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute optimal lambda for PPI++ using closed-form solution. — lambda_hat_ppi_pp_optimal","text":"Optimal lambda (unconstrained).","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/lambda_hat_ppi_pp_optimal.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute optimal lambda for PPI++ using closed-form solution. — lambda_hat_ppi_pp_optimal","text":"equals regression coefficient Y f n >> m.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/llm_point_and_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"LLM-as-a-judge corrected estimator with finite-sample adjustment. — llm_point_and_ci","title":"LLM-as-a-judge corrected estimator with finite-sample adjustment. — llm_point_and_ci","text":"LLM---judge corrected estimator finite-sample adjustment.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/llm_point_and_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"LLM-as-a-judge corrected estimator with finite-sample adjustment. — llm_point_and_ci","text":"","code":"llm_point_and_ci(p_hat, q0_hat, q1_hat, n, m0, m1, alpha = 0.1, eps_J = 1e-04)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/llm_point_and_ci.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"LLM-as-a-judge corrected estimator with finite-sample adjustment. — llm_point_and_ci","text":"p_hat Average LLM score test set. q0_hat Calibration specificity estimate. q1_hat Calibration sensitivity estimate. n Test set size. m0, m1 Calibration sample sizes negatives/positives. alpha Miscoverage level. eps_J Small threshold guard near-random judges.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/llm_point_and_ci.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"LLM-as-a-judge corrected estimator with finite-sample adjustment. — llm_point_and_ci","text":"List theta, var, CI endpoints.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/logit_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"Wald-style confidence interval via logit transform. — logit_ci","title":"Wald-style confidence interval via logit transform. — logit_ci","text":"Wald-style confidence interval via logit transform.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/logit_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wald-style confidence interval via logit transform. — logit_ci","text":"","code":"logit_ci(theta_hat, var_hat, alpha = 0.1)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/logit_ci.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wald-style confidence interval via logit transform. — logit_ci","text":"theta_hat Point estimate probability scale. var_hat Estimated variance theta_hat. alpha Miscoverage level interval.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/logit_ci.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wald-style confidence interval via logit transform. — logit_ci","text":"list lower upper bounds.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/logit_ci_prob.html","id":null,"dir":"Reference","previous_headings":"","what":"Logit-transformed Wald CI for probability parameters. — logit_ci_prob","title":"Logit-transformed Wald CI for probability parameters. — logit_ci_prob","text":"Constructs confidence interval logit scale transforms back, provides better coverage probabilities near 0 1.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/logit_ci_prob.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Logit-transformed Wald CI for probability parameters. — logit_ci_prob","text":"","code":"logit_ci_prob(est, se, level = 0.9)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/logit_ci_prob.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Logit-transformed Wald CI for probability parameters. — logit_ci_prob","text":"est Point estimate probability scale. se Standard error estimate. level Confidence level.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/logit_ci_prob.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Logit-transformed Wald CI for probability parameters. — logit_ci_prob","text":"Numeric vector length 2 (lower, upper) bounds.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/mle_point_and_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"MLE estimator with consistent output format. — mle_point_and_ci","title":"MLE estimator with consistent output format. — mle_point_and_ci","text":"Wrapper around fit_misclass_mle() returns output format estimators (ppi_point_and_ci, rg_point_and_ci, etc.).","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/mle_point_and_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"MLE estimator with consistent output format. — mle_point_and_ci","text":"","code":"mle_point_and_ci(Y_cal, Yhat_cal, Yhat_test, alpha = 0.1)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/mle_point_and_ci.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"MLE estimator with consistent output format. — mle_point_and_ci","text":"Y_cal Vector human labels calibration set (0/1). Yhat_cal Vector surrogate predictions calibration set (0/1). Yhat_test Vector surrogate predictions test set (0/1). alpha Miscoverage level confidence interval.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/mle_point_and_ci.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"MLE estimator with consistent output format. — mle_point_and_ci","text":"list containing: theta Point estimate prevalence. var Estimated variance theta (observed information). ci_lower Lower bound confidence interval. ci_upper Upper bound confidence interval. q0_hat Estimated specificity. q1_hat Estimated sensitivity. full_fit full output fit_misclass_mle advanced use.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/naive_continuous_point_and_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"Naive estimator (just use model predictions) — naive_continuous_point_and_ci","title":"Naive estimator (just use model predictions) — naive_continuous_point_and_ci","text":"Naive estimator (just use model predictions)","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/naive_continuous_point_and_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Naive estimator (just use model predictions) — naive_continuous_point_and_ci","text":"","code":"naive_continuous_point_and_ci(Yhat_test, alpha = 0.1)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/naive_discrete_point_and_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"Naive estimator: just use mean of discrete predictions — naive_discrete_point_and_ci","title":"Naive estimator: just use mean of discrete predictions — naive_discrete_point_and_ci","text":"Naive estimator: just use mean discrete predictions","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/naive_discrete_point_and_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Naive estimator: just use mean of discrete predictions — naive_discrete_point_and_ci","text":"","code":"naive_discrete_point_and_ci(Yhat_test, alpha = 0.1)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/oracle_discrete_point_and_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"Oracle estimator (knows true mu values) — oracle_discrete_point_and_ci","title":"Oracle estimator (knows true mu values) — oracle_discrete_point_and_ci","text":"Oracle estimator (knows true mu values)","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/oracle_discrete_point_and_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Oracle estimator (knows true mu values) — oracle_discrete_point_and_ci","text":"","code":"oracle_discrete_point_and_ci(Yhat_test, mu, alpha = 0.1)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_base_theme.html","id":null,"dir":"Reference","previous_headings":"","what":"Base theme for standard plots — plot_base_theme","title":"Base theme for standard plots — plot_base_theme","text":"Base theme standard plots","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_base_theme.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Base theme for standard plots — plot_base_theme","text":"","code":"plot_base_theme()"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_base_theme.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Base theme for standard plots — plot_base_theme","text":"ggplot2 theme object","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_base_theme_aggregate.html","id":null,"dir":"Reference","previous_headings":"","what":"Larger legend theme for aggregate plots (stacked in two rows) — plot_base_theme_aggregate","title":"Larger legend theme for aggregate plots (stacked in two rows) — plot_base_theme_aggregate","text":"Larger legend theme aggregate plots (stacked two rows)","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_base_theme_aggregate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Larger legend theme for aggregate plots (stacked in two rows) — plot_base_theme_aggregate","text":"","code":"plot_base_theme_aggregate()"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_base_theme_aggregate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Larger legend theme for aggregate plots (stacked in two rows) — plot_base_theme_aggregate","text":"ggplot2 theme object","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_bias.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot estimator bias across simulation settings — plot_bias","title":"Plot estimator bias across simulation settings — plot_bias","text":"Plot estimator bias across simulation settings","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_bias.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot estimator bias across simulation settings — plot_bias","text":"","code":"plot_bias(   data,   facet_formula,   title = \"Estimator Bias\",   subtitle = \"\",   linetype_var = NULL,   use_aggregate = FALSE,   x_breaks = NULL,   point_size = 1.5 )"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_bias.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot estimator bias across simulation settings — plot_bias","text":"data data frame containing summary statistics per configuration. facet_formula faceting formula (e.g. q0_lab ~ q1_lab). title Plot title. subtitle Plot subtitle. linetype_var Optional name column data controls line type. NULL, lines solid. use_aggregate Logical; whether use larger theme suitable aggregate plots many legend entries. x_breaks Optional numeric vector breaks x-axis. point_size Point size scatter overlay.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_bias.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot estimator bias across simulation settings — plot_bias","text":"ggplot object showing bias curves.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_bias_pct.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot percent bias across simulation settings — plot_bias_pct","title":"Plot percent bias across simulation settings — plot_bias_pct","text":"Plot percent bias across simulation settings","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_bias_pct.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot percent bias across simulation settings — plot_bias_pct","text":"","code":"plot_bias_pct(   data,   facet_formula,   title = \"Percent Bias\",   subtitle = \"\",   linetype_var = NULL,   use_aggregate = FALSE,   x_breaks = NULL,   point_size = 1.5 )"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_bias_pct.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot percent bias across simulation settings — plot_bias_pct","text":"data data frame containing summary statistics per configuration. Must include columns theta_true, bias_pct, method. facet_formula faceting formula (e.g. q0_lab ~ q1_lab) specifying arrange simulation panels. title Plot title. subtitle Plot subtitle. linetype_var Optional name column data controls line type. NULL, lines solid. use_aggregate Logical; whether use larger theme suitable aggregate plots many legend entries. x_breaks Optional numeric vector breaks x-axis (theta_true space). point_size Point size scatter overlay.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_bias_pct.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot percent bias across simulation settings — plot_bias_pct","text":"ggplot object showing percent bias curves.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_ci_width.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot mean confidence interval width across simulation settings — plot_ci_width","title":"Plot mean confidence interval width across simulation settings — plot_ci_width","text":"Plot mean confidence interval width across simulation settings","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_ci_width.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot mean confidence interval width across simulation settings — plot_ci_width","text":"","code":"plot_ci_width(   data,   facet_formula,   title = \"CI Width\",   subtitle = \"\",   linetype_var = NULL,   use_aggregate = FALSE,   x_breaks = NULL,   point_size = 1.5,   y_limits = NULL )"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_ci_width.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot mean confidence interval width across simulation settings — plot_ci_width","text":"data data frame containing summary statistics per configuration. Must include columns theta_true, ci_width, method. facet_formula faceting formula (e.g. q0_lab ~ q1_lab) specifying arrange simulation panels. title Plot title. subtitle Plot subtitle. linetype_var Optional name column data controls line type. NULL, lines solid. use_aggregate Logical; whether use larger theme suitable aggregate plots many legend entries. x_breaks Optional numeric vector breaks x-axis (theta_true space). point_size Point size scatter overlay.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_ci_width.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot mean confidence interval width across simulation settings — plot_ci_width","text":"ggplot object showing mean confidence interval width function theta_true.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_coverage.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot empirical coverage across simulation settings — plot_coverage","title":"Plot empirical coverage across simulation settings — plot_coverage","text":"Plot empirical coverage across simulation settings","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_coverage.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot empirical coverage across simulation settings — plot_coverage","text":"","code":"plot_coverage(   data,   facet_formula,   title = \"Coverage\",   subtitle = \"\",   linetype_var = NULL,   use_aggregate = FALSE,   x_breaks = NULL,   point_size = 1.5,   y_limits = NULL,   alpha = 0.9 )"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_coverage.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot empirical coverage across simulation settings — plot_coverage","text":"data data frame containing summary statistics per configuration. Must include columns theta_true, coverage, method. facet_formula faceting formula (e.g. q0_lab ~ q1_lab) specifying arrange simulation panels. title Plot title. subtitle Plot subtitle. linetype_var Optional name column data controls line type. NULL, lines solid. use_aggregate Logical; whether use larger theme suitable aggregate plots many legend entries. x_breaks Optional numeric vector breaks x-axis (theta_true space). point_size Point size scatter overlay. y_limits Optional y-axis limits (c(min, max)). alpha Nominal coverage level horizontal reference line. Defaults 0.9.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_coverage.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot empirical coverage across simulation settings — plot_coverage","text":"ggplot object showing empirical coverage function theta_true, horizontal reference line alpha.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_q_bias.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot calibration bias for \\(\\hat q_0\\) and \\(\\hat q_1\\) — plot_q_bias","title":"Plot calibration bias for \\(\\hat q_0\\) and \\(\\hat q_1\\) — plot_q_bias","text":"Plot calibration bias \\(\\hat q_0\\) \\(\\hat q_1\\)","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_q_bias.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot calibration bias for \\(\\hat q_0\\) and \\(\\hat q_1\\) — plot_q_bias","text":"","code":"plot_q_bias(   data,   facet_formula,   title = \"Calibration Estimate Bias\",   subtitle = \"\",   linetype_var = NULL,   use_aggregate = FALSE,   x_breaks = NULL,   point_size = 1.5 )"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_q_bias.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot calibration bias for \\(\\hat q_0\\) and \\(\\hat q_1\\) — plot_q_bias","text":"data data frame containing calibration summary statistics, typically columns theta_true, bias, metric (e.g. \"q0\" \"q1\"), faceting variables. facet_formula faceting formula (e.g. q0_lab ~ q1_lab q_val_lab ~ label_ratio_lab) specifying panels arranged. title Plot title. subtitle Plot subtitle. linetype_var Optional name column data controls line type. NULL, lines solid. use_aggregate Logical; whether use larger theme suitable aggregate plots many legend entries. x_breaks Optional numeric vector breaks x-axis (theta_true space). point_size Point size scatter overlay.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/plot_q_bias.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot calibration bias for \\(\\hat q_0\\) and \\(\\hat q_1\\) — plot_q_bias","text":"ggplot object showing calibration bias \\(\\hat q_0\\) \\(\\hat q_1\\).","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_continuous_point_and_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"PPI estimator for continuous scores — ppi_continuous_point_and_ci","title":"PPI estimator for continuous scores — ppi_continuous_point_and_ci","text":"PPI estimator continuous scores","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_continuous_point_and_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"PPI estimator for continuous scores — ppi_continuous_point_and_ci","text":"","code":"ppi_continuous_point_and_ci(Y_cal, Yhat_cal, Yhat_test, alpha = 0.1)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_discrete_point_and_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"PPI estimator for discrete predictions theta_hat = mean(Yhat_test) + (mean(Y_cal) - mean(Yhat_cal)) — ppi_discrete_point_and_ci","title":"PPI estimator for discrete predictions theta_hat = mean(Yhat_test) + (mean(Y_cal) - mean(Yhat_cal)) — ppi_discrete_point_and_ci","text":"PPI estimator discrete predictions theta_hat = mean(Yhat_test) + (mean(Y_cal) - mean(Yhat_cal))","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_discrete_point_and_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"PPI estimator for discrete predictions theta_hat = mean(Yhat_test) + (mean(Y_cal) - mean(Yhat_cal)) — ppi_discrete_point_and_ci","text":"","code":"ppi_discrete_point_and_ci(Y_cal, Yhat_cal, Yhat_test, alpha = 0.1)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_multiclass.html","id":null,"dir":"Reference","previous_headings":"","what":"General multiclass PPI estimator with vector-valued g-transform. — ppi_multiclass","title":"General multiclass PPI estimator with vector-valued g-transform. — ppi_multiclass","text":"General multiclass PPI estimator vector-valued g-transform.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_multiclass.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General multiclass PPI estimator with vector-valued g-transform. — ppi_multiclass","text":"","code":"ppi_multiclass(   S_unlabeled,   S_labeled,   Y_labeled,   g_transform,   context,   alpha = 0.05,   project = TRUE )"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_multiclass.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General multiclass PPI estimator with vector-valued g-transform. — ppi_multiclass","text":"S_unlabeled Integer vector surrogate labels (test set). S_labeled Integer vector surrogate labels calibration set. Y_labeled Integer vector true labels calibration set. g_transform Function mapping (S, context) n x K matrix. context List passed g_transform (must include K). alpha Miscoverage level per-class Wald intervals. project Logical; TRUE (default), raw class-prevalence estimates projected onto probability simplex nonnegative sum one.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_multiclass.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"General multiclass PPI estimator with vector-valued g-transform. — ppi_multiclass","text":"list containing per-class prevalence estimates, variance estimates, Wald confidence limits.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_point_and_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"Prediction-powered mean estimator with logit CI. — ppi_point_and_ci","title":"Prediction-powered mean estimator with logit CI. — ppi_point_and_ci","text":"Prediction-powered mean estimator logit CI.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_point_and_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prediction-powered mean estimator with logit CI. — ppi_point_and_ci","text":"","code":"ppi_point_and_ci(Y_L, f_L, f_U, alpha = 0.1)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_point_and_ci.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prediction-powered mean estimator with logit CI. — ppi_point_and_ci","text":"Y_L Vector human labels calibration set. f_L Surrogate predictions calibration set. f_U Surrogate predictions large unlabeled/test set. alpha Miscoverage level confidence interval.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_point_and_ci.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prediction-powered mean estimator with logit CI. — ppi_point_and_ci","text":"List theta, var, CI endpoints.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_pp_continuous_point_and_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"PPI++ estimator for continuous scores — ppi_pp_continuous_point_and_ci","title":"PPI++ estimator for continuous scores — ppi_pp_continuous_point_and_ci","text":"PPI++ estimator continuous scores","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_pp_continuous_point_and_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"PPI++ estimator for continuous scores — ppi_pp_continuous_point_and_ci","text":"","code":"ppi_pp_continuous_point_and_ci(   Y_cal,   Yhat_cal,   Yhat_test,   alpha = 0.1,   lambda_range = c(0, 1) )"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_pp_discrete_point_and_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"PPI++ estimator for discrete predictions — ppi_pp_discrete_point_and_ci","title":"PPI++ estimator for discrete predictions — ppi_pp_discrete_point_and_ci","text":"Uses closed-form optimal lambda (constraints): lambda* = Cov(Y,Yhat) / Var(Yhat_cal) + (m/n) * Var(Yhat_test)","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_pp_discrete_point_and_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"PPI++ estimator for discrete predictions — ppi_pp_discrete_point_and_ci","text":"","code":"ppi_pp_discrete_point_and_ci(Y_cal, Yhat_cal, Yhat_test, alpha = 0.1)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_pp_multiclass.html","id":null,"dir":"Reference","previous_headings":"","what":"Multiclass PPI++ via per-class binary reducers. — ppi_pp_multiclass","title":"Multiclass PPI++ via per-class binary reducers. — ppi_pp_multiclass","text":"Runs scalar PPI++ estimator separately class indicator (1Y=k) using one-hot representation surrogate labels.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_pp_multiclass.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multiclass PPI++ via per-class binary reducers. — ppi_pp_multiclass","text":"","code":"ppi_pp_multiclass(S_unlabeled, S_labeled, Y_labeled, K, alpha = 0.05)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_pp_multiclass.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multiclass PPI++ via per-class binary reducers. — ppi_pp_multiclass","text":"S_unlabeled Surrogate labels large sample. S_labeled Surrogate labels calibration sample. Y_labeled True labels calibration sample. K Number classes. alpha Miscoverage level class.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_pp_multiclass.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multiclass PPI++ via per-class binary reducers. — ppi_pp_multiclass","text":"List per-class prevalence estimates, variance, CI limits, tuned lambda values.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_pp_point_and_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"PPI++ estimator with optimal unconstrained lambda. — ppi_pp_point_and_ci","title":"PPI++ estimator with optimal unconstrained lambda. — ppi_pp_point_and_ci","text":"Uses closed-form optimal lambda: $$\\lambda^* = \\frac{\\text{Cov}(Y, f)}{\\text{Var}(f_L) + (m/n) \\text{Var}(f_U)}$$","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_pp_point_and_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"PPI++ estimator with optimal unconstrained lambda. — ppi_pp_point_and_ci","text":"","code":"ppi_pp_point_and_ci(Y_L, f_L, f_U, alpha = 0.1)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_pp_point_and_ci.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"PPI++ estimator with optimal unconstrained lambda. — ppi_pp_point_and_ci","text":"Y_L Vector true labels calibration set. f_L Vector surrogate predictions calibration set. f_U Vector surrogate predictions test set. alpha Miscoverage level confidence interval.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_pp_point_and_ci.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"PPI++ estimator with optimal unconstrained lambda. — ppi_pp_point_and_ci","text":"List containing theta, lambda, variance, CI limits.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_pp_point_and_ci.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"PPI++ estimator with optimal unconstrained lambda. — ppi_pp_point_and_ci","text":"equivalent using regression coefficient n >> m, provides optimal variance reduction without artificial constraints.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_pp_point_and_ci_general.html","id":null,"dir":"Reference","previous_headings":"","what":"Tuned PPI++ estimator using numeric lambda search. — ppi_pp_point_and_ci_general","title":"Tuned PPI++ estimator using numeric lambda search. — ppi_pp_point_and_ci_general","text":"Tuned PPI++ estimator using numeric lambda search.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_pp_point_and_ci_general.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tuned PPI++ estimator using numeric lambda search. — ppi_pp_point_and_ci_general","text":"","code":"ppi_pp_point_and_ci_general(Y_L, f_L, f_U, alpha = 0.1, lambda_range = c(0, 1))"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_pp_point_and_ci_general.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tuned PPI++ estimator using numeric lambda search. — ppi_pp_point_and_ci_general","text":"Y_L, f_L, f_U Observed labels surrogate scores. alpha Miscoverage level. lambda_range Interval constrain lambda.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_pp_point_and_ci_general.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tuned PPI++ estimator using numeric lambda search. — ppi_pp_point_and_ci_general","text":"List containing theta, lambda, variance, CI limits.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_pp_var_hat.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute PPI++ variance at a given lambda. — ppi_pp_var_hat","title":"Compute PPI++ variance at a given lambda. — ppi_pp_var_hat","text":"Compute PPI++ variance given lambda.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_pp_var_hat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute PPI++ variance at a given lambda. — ppi_pp_var_hat","text":"","code":"ppi_pp_var_hat(lambda, Y_L, f_L, f_U)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_pp_var_hat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute PPI++ variance at a given lambda. — ppi_pp_var_hat","text":"lambda Tuning parameter. Y_L, f_L, f_U Observed labels surrogate scores.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/ppi_pp_var_hat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute PPI++ variance at a given lambda. — ppi_pp_var_hat","text":"Estimated variance.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/project_simplex.html","id":null,"dir":"Reference","previous_headings":"","what":"Euclidean projection onto the probability simplex. — project_simplex","title":"Euclidean projection onto the probability simplex. — project_simplex","text":"Implements standard sorting/threshold algorithm used sparsemax simplex projection: finds closest probability vector unconstrained input L2 norm.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/project_simplex.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Euclidean projection onto the probability simplex. — project_simplex","text":"","code":"project_simplex(v)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/project_simplex.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Euclidean projection onto the probability simplex. — project_simplex","text":"v Numeric vector.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/project_simplex.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Euclidean projection onto the probability simplex. — project_simplex","text":"Numeric vector simplex (nonnegative, sums 1).","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/rg_least_squares_simplex.html","id":null,"dir":"Reference","previous_headings":"","what":"Constrained least-squares Rogan–Gladen estimator. — rg_least_squares_simplex","title":"Constrained least-squares Rogan–Gladen estimator. — rg_least_squares_simplex","text":"Solves quadratic program $$   \\min_{\\pi \\\\Delta_{K-1}} \\left\\| \\hat p - \\hat M \\pi \\right\\|_2^2 $$ enforce simplex structure prevalence vector.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/rg_least_squares_simplex.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Constrained least-squares Rogan–Gladen estimator. — rg_least_squares_simplex","text":"","code":"rg_least_squares_simplex(M_hat, p_hat, jitter = 1e-06)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/rg_least_squares_simplex.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Constrained least-squares Rogan–Gladen estimator. — rg_least_squares_simplex","text":"M_hat Estimated confusion matrix (K x K). p_hat Observed surrogate class probabilities (length K). jitter Small ridge term stabilize quadratic program.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/rg_least_squares_simplex.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Constrained least-squares Rogan–Gladen estimator. — rg_least_squares_simplex","text":"Numeric vector constrained prevalence estimates.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/rg_point_and_ci.html","id":null,"dir":"Reference","previous_headings":"","what":"Rogan-Gladen corrected estimator with finite-sample adjustment. — rg_point_and_ci","title":"Rogan-Gladen corrected estimator with finite-sample adjustment. — rg_point_and_ci","text":"Classical misclassification correction using sensitivity/specificity, also known Rogan-Gladen estimator.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/rg_point_and_ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rogan-Gladen corrected estimator with finite-sample adjustment. — rg_point_and_ci","text":"","code":"rg_point_and_ci(p_hat, q0_hat, q1_hat, n, m0, m1, alpha = 0.1, eps_J = 1e-04)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/rg_point_and_ci.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rogan-Gladen corrected estimator with finite-sample adjustment. — rg_point_and_ci","text":"p_hat Average LLM score test set. q0_hat Calibration specificity estimate. q1_hat Calibration sensitivity estimate. n Test set size. m0, m1 Calibration sample sizes negatives/positives. alpha Miscoverage level. eps_J Small threshold guard near-random judges.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/rg_point_and_ci.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rogan-Gladen corrected estimator with finite-sample adjustment. — rg_point_and_ci","text":"List theta, var, CI endpoints.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/sim_compare_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Monte Carlo diagnostics for the joint MLE. — sim_compare_info","title":"Monte Carlo diagnostics for the joint MLE. — sim_compare_info","text":"Simulates repeated calibration/test splits binary misclassification model study estimator bias, standard errors, interval coverage.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/sim_compare_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Monte Carlo diagnostics for the joint MLE. — sim_compare_info","text":"","code":"sim_compare_info(   B = 1000,   N = 5000,   theta_true = 0.3,   q0_true = 0.8,   q1_true = 0.85,   m_cal = 500,   level = 0.9 )"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/sim_compare_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Monte Carlo diagnostics for the joint MLE. — sim_compare_info","text":"B Number Monte Carlo repetitions. N Total sample size simulate. theta_true, q0_true, q1_true Ground-truth prevalence/specificity/sensitivity. m_cal Size calibration set. level Confidence level coverage diagnostics.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/sim_compare_info.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Monte Carlo diagnostics for the joint MLE. — sim_compare_info","text":"List containing aggregate summaries, coverage, raw simulation draws, ggplot2 objects quick inspection.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/train_model_good.html","id":null,"dir":"Reference","previous_headings":"","what":"Train ","title":"Train ","text":"Knows features matter, uses flexible functional form","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/train_model_good.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Train ","text":"","code":"train_model_good(X, Y)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/train_model_medium.html","id":null,"dir":"Reference","previous_headings":"","what":"Train ","title":"Train ","text":"Uses 10 features including noise features X6-X10 RF flexible partially fit noise","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/train_model_medium.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Train ","text":"","code":"train_model_medium(X, Y)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/train_model_poor_ols.html","id":null,"dir":"Reference","previous_headings":"","what":"Train ","title":"Train ","text":"Uses 10 features, linear form capture nonlinearity + fits noise features NOTE: OLS still guarantees EYhat = EY training distribution!","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/train_model_poor_ols.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Train ","text":"","code":"train_model_poor_ols(X, Y)"},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/train_model_poor_tree.html","id":null,"dir":"Reference","previous_headings":"","what":"Train ","title":"Train ","text":"Uses X1 (relevant) X6 (noise) - severely misspecified KEY: OLS, guarantee EYhat = EY even training data! show bias even without distribution shift.","code":""},{"path":"https://yiqunchen.github.io/debias-llm-as-a-judge/reference/train_model_poor_tree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Train ","text":"","code":"train_model_poor_tree(X, Y)"}]
