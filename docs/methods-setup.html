<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>PPI vs Rogan–Gladen (RG) in Binary Misclassification • debiasLLMReporting</title><script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><script src="deps/MathJax-3.2.2/tex-chtml.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="PPI vs Rogan–Gladen (RG) in Binary Misclassification"></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">debiasLLMReporting</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="articles/getting-started.html">Getting Started</a></li>
<li class="nav-item"><a class="nav-link" href="articles/real-data-example.html">Real Data Example</a></li>
<li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json"></form></li>
      </ul></div>


  </div>
</nav><div class="container template-title-body">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>PPI vs Rogan–Gladen (RG) in Binary Misclassification</h1>

    </div>


<div id="ppi-vs-rogangladen-rg-in-binary-misclassification" class="section level1">

<p>We compare two estimators of a scalar parameter (= [Z] = (Z=1)):</p>
<ol style="list-style-type: decimal"><li>Rogan–Gladen (RG) misclassification‐corrected estimator<br></li>
<li>Vanilla Prediction–Powered Inference (PPI)</li>
</ol><p>and then show how both fit into a more general “PPI with transform (g)” framework.</p>
<hr><div class="section level2">
<h2 id="id_1-setup-and-notation-binary-misclassification">1. Setup and notation (binary misclassification)<a class="anchor" aria-label="anchor" href="#id_1-setup-and-notation-binary-misclassification"></a></h2>
<ul><li>True (human) label: (Z {0,1})</li>
<li>Target parameter: (= [Z] = (Z=1))</li>
<li>Surrogate / judge label: (Z {0,1})</li>
</ul><p>Define:</p>
<ul><li>Sensitivity: [ q_1 = (Z = 1 Z = 1) ]</li>
<li>Specificity: [ q_0 = (Z = 0 Z = 0) ]</li>
<li>Youden’s (J): [ J = q_0 + q_1 - 1 (0,1]  ]</li>
</ul><div class="section level3">
<h3 id="data">Data<a class="anchor" aria-label="anchor" href="#data"></a></h3>
<ul><li><p><strong>Big test set</strong> of size (n): only (Z_i), (i=1,,n) [ p =  _{i=1}^n Z_i p := (Z = 1). ]</p></li>
<li><p><strong>Small calibration set</strong> of size (m): both (Z_j,Z_j), (j=1,,m).</p></li>
</ul><p><strong>Goal</strong>: estimate (= [Z]).</p>
<hr></div>
</div>
<div class="section level2">
<h2 id="id_2-two-estimators">2. Two estimators<a class="anchor" aria-label="anchor" href="#id_2-two-estimators"></a></h2>
<div class="section level3">
<h3 id="id_21-rogangladen-rg-estimator">2.1 Rogan–Gladen (RG) estimator<a class="anchor" aria-label="anchor" href="#id_21-rogangladen-rg-estimator"></a></h3>
<p>Under the standard misclassification model, [ p = (Z = 1) = q_1 + (1-q_0)(1-) = J,+ (1-q_0). ]</p>
<p>Solve for (): [ = . ]</p>
<p>Plug in empirical estimates: - (p = _{i=1}^n Z_i) - (q_0 = ) - (q_1 = )</p>
<p>The RG estimator is: [ _{} = . ]</p>
<p>If we pretend (q_0,q_1) are known (or (m) is huge so their uncertainty is negligible), this is just a linear transform of (p): [ _{} . ]</p>
<hr></div>
<div class="section level3">
<h3 id="id_22-vanilla-ppi-estimator">2.2 Vanilla PPI estimator<a class="anchor" aria-label="anchor" href="#id_22-vanilla-ppi-estimator"></a></h3>
<p>Let: - (Y := Z) - (f(X) := Z) (we ignore (X) explicitly; treat (Z) as a function of data)</p>
<p>Then: [  = [Z] = [Z] - [Z - Z] = p - , ] where [ := [Z - Z]. ]</p>
<p>Estimators:</p>
<ul><li><p>On the test set: [ p =  _{i=1}^n Z_i. ]</p></li>
<li><p>On the calibration set: [  =  _{j=1}^m (Z_j - Z_j). ]</p></li>
</ul><p>PPI estimator: [ _{} = p - . ]</p>
<p>This uses only the <em>average prediction error</em> from the calibration set; it never explicitly uses (q_0,q_1).</p>
<hr></div>
</div>
<div class="section level2">
<h2 id="id_3-asymptotic-variances-simple-regime">3. Asymptotic variances (simple regime)<a class="anchor" aria-label="anchor" href="#id_3-asymptotic-variances-simple-regime"></a></h2>
<p>Assumptions for this comparison:</p>
<ul><li>Test and calibration sets are i.i.d. from the same population.</li>
<li>(n) (test size) and (m) (calibration size) are both large.</li>
<li>Treat ((q_0,q_1)), and hence (J), as fixed constants (ignoring their estimation variance for now).</li>
</ul><div class="section level3">
<h3 id="id_31-variance-of-rg-estimator-known-q_0q_1">3.1 Variance of RG estimator (known (q_0,q_1))<a class="anchor" aria-label="anchor" href="#id_31-variance-of-rg-estimator-known-q_0q_1"></a></h3>
<p>If (q_0,q_1) are known, then [ _{} =  ] is linear in (p).</p>
<p>Since (p) is a binomial proportion: [ (p) , ] we obtain: [ (_{}) .  ]</p>
<p>This is the usual binomial variance scaled by (1/J^2).</p>
<hr></div>
<div class="section level3">
<h3 id="id_32-variance-of-ppi-estimator">3.2 Variance of PPI estimator<a class="anchor" aria-label="anchor" href="#id_32-variance-of-ppi-estimator"></a></h3>
<p>General result for mean PPI (Angelopoulos et al.):</p>
<p>If () is constructed from: - a large “unlabeled” set of size (n_{}) with only (f(X)), - a “labeled” set of size (n_{}) with both (f(X)) and (Y),</p>
<p>then asymptotically: [ (_{})   + . ]</p>
<p>In our case: - (f(X) = Z), - (Y = Z), - (n_{}=n) (test set), - (n_{}=m) (calibration set),</p>
<p>so: [ (_{})   + .  ]</p>
<p>We have: [ (Z) = p(1-p). ]</p>
<p>Also, (Z - Z {-1,0,1}):</p>
<ul><li>(Z - Z = +1) for false positives (FP),</li>
<li>(Z - Z = -1) for false negatives (FN),</li>
<li>(Z - Z = 0) otherwise.</li>
</ul><p>Let: - (() = (Z = 1, Z = 0)), - (() = (Z = 0, Z = 1)).</p>
<p>Let (= [Z - Z] = p - ). Then: [ (Z - Z) = () + () - ^2. ]</p>
<p>Therefore: [ (_{})   + . ]</p>
<p>If (m) is large enough that the calibration term is negligible: [ (_{}) . ]</p>
<hr></div>
<div class="section level3">
<h3 id="id_33-direct-comparison-when-calibration-is-large">3.3 Direct comparison when calibration is large<a class="anchor" aria-label="anchor" href="#id_33-direct-comparison-when-calibration-is-large"></a></h3>
<p>In the “large calibration” regime (ignore the calibration term in (PPI)):</p>
<ul><li><p>PPI variance: [ (_{}) . ]</p></li>
<li><p>RG variance: [ (_{}) . ]</p></li>
</ul><p>Since (0 &lt; J ), we have (1/J^2 ), with strict inequality whenever the judge is imperfect (i.e., (J &lt; 1)). Thus: [ (<em>{})  (</em>{}), ] with equality only if the judge is perfect ((J = 1)).</p>
<p>With <strong>finite</strong> (m), PPI gains the additional term ((Z - Z)/m), and RG gains extra variance from estimating (q_0,q_1) (not written out here), so there is no uniform dominance in all parameter regimes. But the structural difference remains:</p>
<ul><li>RG amplifies test‑set noise by (1/J^2),</li>
<li>PPI keeps the test‑set noise at the base (p(1-p)/n) and adds a separate calibration term.</li>
</ul><hr></div>
</div>
<div class="section level2">
<h2 id="id_4-ppi-with-a-general-transform-g">4. PPI with a general transform (g)<a class="anchor" aria-label="anchor" href="#id_4-ppi-with-a-general-transform-g"></a></h2>
<p>Let (S) denote the surrogate (here (S = Z)), and let (X) be any additional features (possibly ignored).</p>
<p>For <strong>any</strong> function (g(S,X)), define the PPI‑style estimator: [ <em>g = </em>{} - _{}. ]</p>
<p>Then: [ [_g] = [g(S,X)] - [g(S,X) - Z] = [Z] = . ]</p>
<p>Asymptotic variance: [ (_g)   + .  ]</p>
<p>Choosing (g) is, therefore, a variance‑optimization problem.</p>
<div class="section level3">
<h3 id="id_41-special-choices-of-g">4.1 Special choices of (g)<a class="anchor" aria-label="anchor" href="#id_41-special-choices-of-g"></a></h3>
<div class="section level4">
<h4 id="a-identity--vanilla-ppi">(a) Identity / vanilla PPI<a class="anchor" aria-label="anchor" href="#a-identity--vanilla-ppi"></a></h4>
<p>[ g_{}(S) = S = Z. ]</p>
<p>Then: [ (_{})   + . ]</p>
</div>
<div class="section level4">
<h4 id="b-rogangladen-transform-as-g">(b) Rogan–Gladen transform as (g)<a class="anchor" aria-label="anchor" href="#b-rogangladen-transform-as-g"></a></h4>
<p>Suppose you have fixed or pilot estimates (q_0,q_1) of ((q_0,q_1)), and define (J = q_0 + q_1 - 1).</p>
<p>Define: [ g_{}(S) = . ]</p>
<p>Then: [  <em>{} g</em>{}(S_i) = , ] which is the RG plug‑in estimator with (q_0,q_1) in place of (q_0,q_1).</p>
<p>If you <strong>stop here</strong> (omit the rectifier), you recover the usual RG‑style estimator (with fixed (q_0,q_1)).</p>
<p>With full PPI: [ <em>{g</em>{}} =  <em>{} g</em>{}(S_i) -  <em>{} ( g</em>{}(S_j) - Z_j ), ] which is still unbiased for () and uses the calibration set to correct any residual bias in (g_{}).</p>
</div>
<div class="section level4">
<h4 id="c-learned--optimal-g">(c) Learned / optimal (g)<a class="anchor" aria-label="anchor" href="#c-learned--optimal-g"></a></h4>
<p>More generally, you can <strong>learn</strong> (g) on the calibration set (e.g., via regression) so that (g(S,X)) approximates (Z) as well as possible, thereby minimizing the variance in ((*)). RePPI shows how to construct such an asymptotically optimal (g) via influence‑function regression.</p>
<hr></div>
</div>
</div>
<div class="section level2">
<h2 id="id_5-summary">5. Summary<a class="anchor" aria-label="anchor" href="#id_5-summary"></a></h2>
<ul><li><p><strong>RG estimator</strong>: [ _{} = . ]</p></li>
<li>
<p><strong>Vanilla PPI estimator (with (g(S)=S))</strong>: [ <em>{} = p -  =  </em>{} Z_i</p>
<ul><li> _{} (Z_j - Z_j). ]</li>
</ul></li>
<li>
<p><strong>General PPI with transform (g)</strong>: [ <em>g =  </em>{} g(S_i,X_i)</p>
<ul><li> _{} (g(S_j,X_j) - Z_j). ]</li>
</ul></li>
</ul><p>In the large‑calibration limit and when the judge is imperfect ((J&lt;1)), PPI has asymptotic variance (p(1-p)/n), whereas RG has variance (p(1-p)/(nJ^2)); thus, PPI is asymptotically more efficient in that regime.</p>
<hr></div>
</div>
<div class="section level1">
<h1 id="multiclass-k-class-misclassification-rg-vs-ppi">Multiclass (K-Class) Misclassification: RG vs PPI<a class="anchor" aria-label="anchor" href="#multiclass-k-class-misclassification-rg-vs-ppi"></a></h1>
<p>We now consider a <strong>K-class</strong> categorical outcome, observed through a noisy surrogate (e.g., an LLM labeler or classifier). We compare:</p>
<ul><li>A <strong>multiclass Rogan–Gladen / confusion-matrix inversion</strong> estimator, and<br></li>
<li>A <strong>Prediction-Powered Inference (PPI)</strong> estimator,</li>
</ul><p>and then show how both fit into a general PPI-with-transform-(g) framework.</p>
<div class="section level2">
<h2 id="id_1-setup-and-notation-k-class-categorical-misclassification">1. Setup and Notation (K-Class Categorical Misclassification)<a class="anchor" aria-label="anchor" href="#id_1-setup-and-notation-k-class-categorical-misclassification"></a></h2>
<p>Let the true label be (Y {1,,K}) with target class probabilities (= (_1,,<em>K)^). The surrogate (S{1,,K}) obeys a misclassification matrix (M) with entries (M</em>{ab} = (S=a Y=b)) and observed surrogate probabilities (p = (p_1,,p_K)^) satisfying (p = M) (assume (M) invertible).</p>
<p>Data:</p>
<ul><li>
<strong>Large test set</strong> of size (n): observe (S_i) only; () is the empirical class distribution of (S).</li>
<li>
<strong>Calibration set</strong> of size (m): observe ((Y_j,S_j)); estimate (M) via (M_{ab} = #{j:S_j=a,Y_j=b} / #{j:Y_j=b}).</li>
</ul><p>Goal: estimate ().</p>
</div>
<div class="section level2">
<h2 id="id_2-two-estimators-1">2. Two Estimators<a class="anchor" aria-label="anchor" href="#id_2-two-estimators-1"></a></h2>
<div class="section level3">
<h3 id="id_21-multiclass-rogangladen--confusion-matrix-inversion">2.1 Multiclass Rogan–Gladen / Confusion-Matrix Inversion<a class="anchor" aria-label="anchor" href="#id_21-multiclass-rogangladen--confusion-matrix-inversion"></a></h3>
<p>Since (p = M), the true prevalence vector is (= M^{-1}p). Plugging in (M) and () yields: [ <em>{} = M^{-1} . ] Componentwise (</em>{,k}) is the (k)-th entry of (M^{-1}). This generalizes binary RG by inverting the estimated confusion matrix.</p>
</div>
<div class="section level3">
<h3 id="id_22-ppi-estimator-identity-g-class-by-class">2.2 PPI Estimator (Identity (g), Class-by-Class)<a class="anchor" aria-label="anchor" href="#id_22-ppi-estimator-identity-g-class-by-class"></a></h3>
<p>Represent (Y) via one-hot vectors (Y = (1{Y=1},,1{Y=K})^), so (= [Y]). For class (k), set (g<sup>{}<em>k(S) = 1{S=k}) and apply PPI: [ <sup>{}<em>k = </em>{i=1}</sup>n g<sup>{}<em>k(S_i) + </em>{j=1}</sup>m (Y_j^{(k)} - g^{}<em>k(S_j)). ] Stacking gives: [ </em>{} = </em>{i=1}</sup>n g^{}(S_i) + _{j=1}^m (Y_j - g^{}(S_j)). ]</p>
</div>
</div>
<div class="section level2">
<h2 id="id_3-asymptotic-variances-sketch">3. Asymptotic Variances (Sketch)<a class="anchor" aria-label="anchor" href="#id_3-asymptotic-variances-sketch"></a></h2>
<p>Treat estimators as vectors in (^K).</p>
<div class="section level3">
<h3 id="id_31-rg-estimator-known-m">3.1 RG Estimator (Known (M))<a class="anchor" aria-label="anchor" href="#id_31-rg-estimator-known-m"></a></h3>
<p>If (M) is known, (<em>{} = M^{-1}) with multinomial covariance: [ (</em>{}) , M^{-1}((p) - pp<sup>)(M</sup>{-1})^. ] Estimating (M) adds a delta-method variance term.</p>
</div>
<div class="section level3">
<h3 id="id_32-ppi-estimator-vector-form">3.2 PPI Estimator (Vector Form)<a class="anchor" aria-label="anchor" href="#id_32-ppi-estimator-vector-form"></a></h3>
<p>For any vector-valued (g(S)): [ <em>g = </em>{i=1}^n g(S_i) + _{j=1}^m (Y_j - g(S_j)), ] with [ (_g) ,(g(S)) + ,(Y - g(S)). ] Setting (g = g^{}) produces the classwise PPI variances. Other (g) choices target lower variance.</p>
</div>
</div>
<div class="section level2">
<h2 id="id_4-ppi-with-transform-g-multiclass">4. PPI with Transform (g) (Multiclass)<a class="anchor" aria-label="anchor" href="#id_4-ppi-with-transform-g-multiclass"></a></h2>
<p>Let (g(S,X)^K). Then [ <em>g = </em>{i=1}^n g(S_i,X_i) + _{j=1}^m (Y_j - g(S_j,X_j)) ] remains unbiased with covariance ((g) + (Y-g)). Examples:</p>
<ol style="list-style-type: decimal"><li>
<strong>Identity / naïve PPI</strong>: (g^{}(S)) (one-hot).<br></li>
<li>
<strong>Confusion-matrix transform (RG)</strong>: with (W=M^{-1}), set (g^{}(S)=W e_S) (column (S) of (W)). The plug-in term alone recover RG; adding the rectifier keeps unbiasedness while reducing variance.<br></li>
<li>
<strong>Soft/learned (g)</strong>: fit (g_k(S,X)(Y=kS,X)), e.g., via multinomial logistic regression, and plug into the same formula.</li>
</ol><p>This shows RG is a special linear (g), while PPI can incorporate RG or learned transforms to trade off bias and variance flexibly.</p>
<div class="section level3">
<h3 id="id_42-projection-onto-the-simplex">4.2 Projection onto the simplex<a class="anchor" aria-label="anchor" href="#id_42-projection-onto-the-simplex"></a></h3>
<p>Rather than ad-hoc clipping of each coordinate, we can enforce the probability-simplex constraints on any multiclass estimator by projecting: [  = <em>{</em>{K-1}} |- |<em>2^2,  </em>{K-1} = {: <em>k , </em>{k=1}^K _k = 1}. ] The minimizer has a closed-form “sorting + threshold” solution identical to the sparsemax/simplex-projection algorithm. It returns the closest valid probability vector to the unconstrained estimate and is standard in quantification and survey calibration workflows.</p>
</div>
<div class="section level3">
<h3 id="id_43-constrained-rg--constrained-likelihood">4.3 Constrained RG / constrained likelihood<a class="anchor" aria-label="anchor" href="#id_43-constrained-rg--constrained-likelihood"></a></h3>
<p>On the RG side, instead of computing (M^{-1} ) directly, we can solve the constrained least-squares problem [ <em>{</em>{K-1}} | - M |_2^2, ] or the constrained multinomial likelihood. This enforces non-negativity and the sum-to-one constraint “inside” the estimation, and can be viewed as RG with a simplex constraint baked in at the parameter level. The projection-based RG and the constrained least-squares RG coincide when (M) is invertible and the solution lies in the simplex interior, but differ when inversion would leave the simplex. Both are implemented in the companion code so you can compare unconstrained, projected, and fully constrained variants.</p>
</div>
</div>
</div>


  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by <a href="https://yiqunchen.github.io/" class="external-link">Yiqun Chen</a>, <a href="https://github.com/Raymond2Moran" class="external-link">Moran Guo</a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer></div>





  </body></html>

