\documentclass[11pt]{article}

\usepackage{amsmath, amssymb, amsthm, bm}
\usepackage{fullpage}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{graphicx}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue
}

% ==========================
% Macros
% ==========================
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prb}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\ind}[1]{\mathbf{1}\{#1\}}

\newcommand{\Zhat}{\hat{Z}}
\newcommand{\Yhat}{\hat{Y}}
\newcommand{\thetahat}{\hat{\theta}}
\newcommand{\thetabar}{\bar{\theta}}
\newcommand{\pihat}{\hat{\bm{\pi}}}
\newcommand{\Mhat}{\hat{M}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}

\begin{document}

\title{Debiasing LLM-as-a-Judge Evaluations via Prediction-Powered Inference and Measurement-Error Correction}
\author{Author Name\\
  Institution\\
  \texttt{email@domain}}
\date{\today}
\maketitle

\begin{abstract}
Large language models (LLMs) are increasingly used as automatic evaluators of other models, playing the role of ``LLM-as-a-judge.'' 
However, LLM judges are noisy surrogates for human judgment and can exhibit systematic bias.
In this paper we formalize the LLM-as-a-judge evaluation problem as a measurement-error problem, and study two classes of debiasing methods:
(i) direct measurement-error correction based on misclassification models, including Rogan--Gladen-type estimators, and
(ii) prediction-powered inference (PPI) estimators (including the recent PPI++ shrinkage variant) that treat LLM predictions as surrogate outcomes and correct their bias using a small set of gold-standard human labels.
We provide a unified framework for binary and multi-class (multinomial) evaluation targets, derive asymptotic variances for the competing estimators, and highlight regimes in which PPI-type estimators can be more efficient than direct misclassification correction.
We illustrate the methods through simulations and a real LLM evaluation case study.
\end{abstract}

\section{Introduction}
\label{sec:intro}

\subsection{LLM-as-a-Judge and Noisy Evaluation}

Recent work proposes to use LLMs as automatic judges to evaluate the outputs of other models, for example by deciding whether a model's answer is correct, assigning a quality score, or choosing between two model responses.
This promises scalable evaluation at low cost, but introduces a new source of error: the LLM judge is an imperfect proxy for human judgment.

Suppose we wish to estimate the human-level accuracy of a base model on some evaluation distribution.
In an ideal world, we would label every example with human annotators.
Instead, LLM-as-a-judge workflows typically rely on:
\begin{itemize}[leftmargin=1.5em]
  \item a large \emph{evaluation set} on which only LLM judgments are available, and
  \item a smaller \emph{calibration set} on which both human labels and LLM judgments are observed.
\end{itemize}
We then ask: how can we use these two datasets to construct valid, efficient estimates and confidence intervals for human-level evaluation metrics?

\subsection{Measurement Error and Prediction-Powered Inference}

Statistically, this is a classical measurement-error problem:
\begin{itemize}[leftmargin=1.5em]
  \item the human label $Z$ is the latent ``truth'',
  \item the LLM judgment $\Zhat$ is a noisy surrogate,
  \item and we only observe $\Zhat$ on most of the evaluation set.
\end{itemize}
One natural approach is to model the \emph{misclassification mechanism} $\Prb(\Zhat \mid Z)$ and then debias the observed LLM accuracy via direct measurement-error correction (e.g.\ Rogan--Gladen-type estimators).

An alternative is \emph{prediction-powered inference} (PPI) \citep{angelopoulos2023prediction, ji2025predictions}, which treats the LLM judgment (or a transform thereof) as a generic surrogate outcome $S$ for $Z$, and uses a small labeled subset to correct the average bias of $S$.
PPI provides a general recipe for combining a large pool of predictions with a small pool of true labels to obtain valid inference, without committing to any particular parametric misclassification model. More elaborate learned-$g$ variants (e.g., RePPI) are intriguing but treated as future extensions rather than part of our benchmark.

This paper situates LLM-as-a-judge evaluation within both perspectives:
we formalize binary and multinomial evaluation targets, describe direct misclassification-correction estimators and PPI-type estimators in a common notation, and compare their asymptotic efficiency.

\subsection{Contributions}

At a high level, our contributions are:
\begin{enumerate}[leftmargin=1.5em]
  \item We propose a unified model for LLM-as-a-judge evaluation that covers binary and multinomial (multi-class) outcomes, and connects directly to classical misclassification and quantification problems.
  \item We formalize two classes of debiasing methods:
  \begin{itemize}
    \item \emph{Direct measurement-error correction}, including Rogan--Gladen and confusion-matrix inversion.
    \item \emph{Prediction-powered inference}, including PPI++ shrinkage, with LLM judgments as surrogates.
  \end{itemize}
  \item For both binary and multinomial targets, we derive asymptotic variances for the competing estimators and characterize regimes in which PPI-type estimators can have lower variance than direct misclassification correction.
  \item We outline a simulation framework and a real-data application to LLM-as-a-judge accuracy estimation, to be filled in with empirical results.
\end{enumerate}

\section{Problem Setup and Models}
\label{sec:setup}

In this section we formalize the evaluation targets and the data-generating mechanisms for binary and $K$-class outcomes.

\subsection{Data Structure}

We consider an evaluation distribution over instances $X$ (e.g.\ prompts, questions, and model outputs).
For each instance $i$, let:
\begin{itemize}[leftmargin=1.5em]
  \item $Z_i$ be the human ground-truth label or evaluation outcome.
  \item $S_i$ be a surrogate or LLM-based quantity constructed from the instance, such as:
  \begin{itemize}
    \item a binary LLM verdict $\Zhat_i \in \{0,1\}$ (correct / incorrect),
    \item a $K$-class label $S_i \in \{1,\dots,K\}$,
    \item or a vector of scores (e.g.\ class probabilities, continuous ratings).
  \end{itemize}
\end{itemize}

We observe:
\begin{itemize}[leftmargin=1.5em]
  \item a large ``test'' sample (index set $U$) of size $n$, where we only see $(X_i, S_i)$;
  \item a smaller ``calibration'' sample (index set $L$) of size $m$, where we see $(X_j, S_j, Z_j)$.
\end{itemize}
We assume $(X_i, Z_i, S_i)$ are i.i.d.\ from a common distribution, and that $L$ is a subsample of $U$ (or another sample from the same population).

\subsection{Binary Outcome Model}
\label{subsec:binary-model}

We first consider the case where $Z$ is binary:
\[
Z \in \{0,1\}, \qquad \theta := \Prb(Z = 1) = \E[Z].
\]

Here $\theta$ might represent, for example, the human accuracy of a model on the evaluation distribution (the probability that the model's answer is correct according to human judges).

\subsubsection{LLM Judge as a Misclassified Binary Outcome}

Suppose the LLM judge outputs a binary decision $\Zhat \in \{0,1\}$ (``correct'' / ``incorrect'').
Define:
\[
q_1 := \Prb(\Zhat = 1 \mid Z = 1)
\quad\text{(sensitivity)}, \qquad
q_0 := \Prb(\Zhat = 0 \mid Z = 0)
\quad\text{(specificity)}.
\]
Let $p := \Prb(\Zhat=1)$ denote the marginal probability that the LLM calls an instance ``correct''.
Then
\begin{equation}
  p = q_1 \theta + (1-q_0)(1-\theta)
    = (q_0 + q_1 - 1)\,\theta + (1-q_0).
  \label{eq:binary-misclass}
\end{equation}
This is the classical binary misclassification model; in epidemiology, $Z$ is disease prevalence and $\Zhat$ is the result of an imperfect diagnostic test \citep{rogan1978estimating, lang2014confidence}.

\subsection{Multinomial ($K$-Class) Outcome Model}
\label{subsec:multinomial-model}

Now let $Z$ take values in a finite set of $K$ classes:
\[
Z \in \{1,\dots,K\}.
\]
Let
\[
\bm{\pi} := (\pi_1,\dots,\pi_K)^\top,
\qquad
\pi_k := \Prb(Z = k),
\]
be the vector of true class proportions.

Suppose the LLM judge outputs a $K$-class label $S \in \{1,\dots,K\}$.
Define the $K\times K$ \emph{misclassification matrix}
\[
M_{ab} := \Prb(S = a \mid Z = b),
\qquad a,b \in \{1,\dots,K\}.
\]
The observed surrogate class proportions
\[
p_a := \Prb(S = a),
\qquad \bm{p} = (p_1,\dots,p_K)^\top,
\]
satisfy the linear system
\begin{equation}
  \bm{p} = M \bm{\pi}.
  \label{eq:multiclass-misclass}
\end{equation}
When $M$ is invertible, the true class proportions are
$\bm{\pi} = M^{-1}\bm{p}$.

This is the standard model in multicategory misclassification and quantification \citep[e.g.][]{rogan1978estimating, fiksel2022generalized}.

\section{Methods}
\label{sec:methods}

We now describe the estimators we consider for debiasing LLM-as-a-judge evaluation.

\subsection{Direct Measurement-Error Correction (Rogan--Gladen and Confusion-Matrix Inversion)}
\label{subsec:rg}

\subsubsection{Binary Case}

In the binary model~\eqref{eq:binary-misclass}, solving for $\theta$ yields
\begin{equation}
  \theta = \frac{p + q_0 - 1}{q_0 + q_1 - 1} 
        = \frac{p + q_0 - 1}{J},
  \qquad J := q_0 + q_1 - 1.
  \label{eq:binary-rg}
\end{equation}
This is the classical Rogan--Gladen estimator for prevalence correction \citep{rogan1978estimating}.

In practice, $p, q_0, q_1$ are unknown and must be estimated from the data:
\begin{itemize}[leftmargin=1.5em]
  \item From the test set $U$:
  \[
  \hat{p} = \frac{1}{|U|} \sum_{i\in U} \Zhat_i.
  \]
  \item From the calibration set $L$:
  \[
  \hat{q}_1 = \frac{\sum_{j\in L} \ind{\Zhat_j=1,\,Z_j=1}}{\sum_{j\in L}\ind{Z_j=1}}, 
  \qquad
  \hat{q}_0 = \frac{\sum_{j\in L} \ind{\Zhat_j=0,\,Z_j=0}}{\sum_{j\in L}\ind{Z_j=0}}.
  \]
\end{itemize}
Plugging these into \eqref{eq:binary-rg} gives the Rogan--Gladen estimator
\begin{equation}
  \hat{\theta}_{\mathrm{RG}}
    = \frac{\hat{p} + \hat{q}_0 - 1}{\hat{q}_0 + \hat{q}_1 - 1}.
  \label{eq:binary-rg-hat}
\end{equation}
\citet{lang2014confidence} discuss confidence intervals that account for the sampling variability in both $\hat{p}$ and $(\hat{q}_0,\hat{q}_1)$.

\subsubsection{Multinomial Case}

In the $K$-class model~\eqref{eq:multiclass-misclass}, the natural generalization is:
\begin{itemize}[leftmargin=1.5em]
  \item From the test set $U$:
  \[
  \hat{p}_a = \frac{1}{|U|} \sum_{i\in U} \ind{S_i = a},
  \qquad
  \hat{\bm{p}} = (\hat{p}_1, \dots, \hat{p}_K)^\top.
  \]
  \item From the calibration set $L$:
  \[
  \hat{M}_{ab}
    =
    \frac{\sum_{j\in L} \ind{S_j = a,\,Z_j = b}}
         {\sum_{j\in L} \ind{Z_j = b}}.
  \]
\end{itemize}
Assuming $\hat{M}$ is invertible, the \emph{multiclass RG / confusion-matrix inversion} estimator is
\begin{equation}
  \hat{\bm{\pi}}_{\mathrm{RG}}
    = \hat{M}^{-1}\, \hat{\bm{p}}.
  \label{eq:multi-rg}
\end{equation}
As in the binary case, this estimator is unbiased under the misclassification model, but can exhibit high variance or leave the simplex in finite samples.

\subsection{Prediction-Powered Inference (PPI) for Binary Outcomes}
\label{subsec:ppi-binary}

Prediction-powered inference treats a surrogate prediction as a biased proxy for the true outcome and corrects its bias using a labeled subset \citep{angelopoulos2023prediction}.

In the binary LLM-as-a-judge setting, take:
\[
Y := Z \in \{0,1\}, \qquad \theta = \E[Y],
\]
and consider a surrogate $S$ constructed from the LLM judgment (e.g.\ $S = \Zhat$ or a transform thereof).
PPI uses the identity
\begin{equation}
  \theta
    = \E[Y]
    = \E[S] - \E[S - Y]
    = \mu - \Delta,
  \label{eq:ppi-identity-binary}
\end{equation}
where $\mu := \E[S]$ and $\Delta := \E[S-Y]$.

We estimate:
\begin{align*}
  \hat{\mu} 
    &= \frac{1}{|U|} \sum_{i\in U} S_i,\\
  \hat{\Delta}
    &= \frac{1}{|L|} \sum_{j\in L} (S_j - Y_j),
\end{align*}
and define the PPI estimator
\begin{equation}
  \hat{\theta}_{\mathrm{PPI}} = \hat{\mu} - \hat{\Delta}.
  \label{eq:ppi-binary}
\end{equation}
This estimator is unbiased for $\theta$ regardless of the distribution of $(S,Y)$, so long as $U$ and $L$ are drawn from the same population.

\subsection{PPI with a General Transform $g$ (Future extensions)}
\label{subsec:ppi-g}

More generally, let $g(S,X)$ be any function of the surrogate $S$ and features $X$.
Define the \emph{PPI-with-transform} estimator
\begin{equation}
  \hat{\theta}_g
    =
    \frac{1}{|U|}\sum_{i\in U} g(S_i,X_i)
    -
    \frac{1}{|L|}\sum_{j\in L} \big(g(S_j,X_j) - Y_j\big).
  \label{eq:ppi-g}
\end{equation}
It is straightforward to check that $\E[\hat{\theta}_g] = \theta$.

In the RePPI framework \citep{ji2025predictions}, $g$ is chosen (or learned) to minimize the asymptotic variance of $\hat{\theta}_g$, leading to an asymptotically optimal ``imputed loss'' function. We do not benchmark learned $g$ functions here; deriving and tuning them for LLM-as-a-judge settings is left to future work.

\subsection{Multinomial PPI for Class Proportions}
\label{subsec:ppi-multinomial}

For the $K$-class problem, encode $Z$ as a one-hot vector:
\[
\bm{Y} := e(Z) \in \{0,1\}^K,
\qquad
\bm{\pi} = \E[\bm{Y}].
\]
Let $g(S,X)\in\R^K$ denote a vector-valued function of the surrogate and features.
The multinomial PPI estimator is
\begin{equation}
  \hat{\bm{\pi}}_g
    =
    \frac{1}{|U|}\sum_{i\in U} g(S_i,X_i)
    +
    \frac{1}{|L|}\sum_{j\in L} \big(\bm{Y}_j - g(S_j,X_j)\big).
  \label{eq:ppi-multinomial}
\end{equation}
Again, $\E[\hat{\bm{\pi}}_g] = \bm{\pi}$ for any $g$.

Special cases include:
\begin{itemize}[leftmargin=1.5em]
  \item \textbf{Naive PPI}: $g(S) = e(S)$, the one-hot of the surrogate label.
  \item \textbf{RG-based $g$}: $g(S) = \Mhat^{-1} e(S)$, which recovers \eqref{eq:multi-rg} if the rectifier term is omitted.
  \item \textbf{Learned $g$ (future work)}: $g$ estimated by regressing $\bm{Y}$ on $(S,X)$ in the calibration sample, as in RePPI.
\end{itemize}

\section{Asymptotic Variance and Efficiency}
\label{sec:theory}

In this section we derive asymptotic variances for the RG and PPI-type estimators and compare their efficiency.

\subsection{Binary Case}
\label{subsec:theory-binary}

We sketch the main formulas; full conditions and proofs can be added as needed.

\subsubsection{Rogan--Gladen}

For simplicity, suppose $q_0,q_1$ are treated as known constants and only $\hat{p}$ contributes sampling variance, i.e.\ we ignore the uncertainty in $(\hat{q}_0,\hat{q}_1)$.
From \eqref{eq:binary-rg},
\[
\hat{\theta}_{\mathrm{RG}} = \frac{\hat{p} + q_0 - 1}{J},
\]
with $J = q_0 + q_1 - 1$.
Since $\hat{p}$ is a binomial proportion,
\[
\Var(\hat{p}) \approx \frac{p(1-p)}{|U|},
\]
we obtain
\begin{equation}
  \Var(\hat{\theta}_{\mathrm{RG}})
    \approx \frac{p(1-p)}{|U| J^2}.
  \label{eq:var-rg-binary}
\end{equation}
Accounting for the variance in $\hat{q}_0,\hat{q}_1$ requires a multivariate delta method.

\subsubsection{PPI with Identity $g$}

In the binary case, with $S = \Zhat$ and $Y=Z$, the PPI estimator is
\[
\hat{\theta}_{\mathrm{PPI}} = \hat{\mu} - \hat{\Delta},
\]
where
\[
\hat{\mu} = \frac{1}{|U|}\sum_{i\in U} \Zhat_i, \qquad
\hat{\Delta} = \frac{1}{|L|}\sum_{j\in L} (\Zhat_j - Z_j).
\]
Under mild regularity, a central limit theorem yields
\begin{equation}
  \Var(\hat{\theta}_{\mathrm{PPI}})
    \approx \frac{\Var(\Zhat)}{|U|}
          + \frac{\Var(\Zhat - Z)}{|L|}.
  \label{eq:var-ppi-binary}
\end{equation}
Here $\Var(\Zhat) = p(1-p)$, and $\Var(\Zhat - Z)$ can be expressed in terms of the probabilities of false positives and false negatives.

Note that if $|L|$ is large so that the second term is negligible, then
\[
\Var(\hat{\theta}_{\mathrm{PPI}})
  \approx \frac{p(1-p)}{|U|},
\]
while $\Var(\hat{\theta}_{\mathrm{RG}}) \approx p(1-p)/(|U|J^2)$.
Since $J^2 \le 1$, RG inflates the test-set variance by $1/J^2$.

\subsubsection{General PPI with Transform $g$}

For a general scalar $g(S,X)$, the PPI estimator \eqref{eq:ppi-g} satisfies
\begin{equation}
  \Var(\hat{\theta}_g)
    \approx
    \frac{\Var(g(S,X))}{|U|}
    +
    \frac{\Var(g(S,X) - Y)}{|L|}.
  \label{eq:var-ppi-g}
\end{equation}
Future learned-$g$ approaches (e.g., the RePPI objective) minimize this variance over a suitable function class; we defer such work to future extensions.

\subsection{Variance comparison of RG vs PPI}
\label{subsec:var-comparison}

The preceding expressions make it easy to compare the two estimators. In the binary misclassification model~\eqref{eq:binary-misclass}, treating $(q_0,q_1)$ as fixed, the RG estimator is just a rescaled version of $\hat{p}$ and has variance
\begin{equation}
  \Var(\hat{\theta}_{\mathrm{RG}})
    \approx \frac{p(1-p)}{|U|J^2},
  \qquad J = q_0 + q_1 - 1.
  \label{eq:var-rg-binary-closed}
\end{equation}
The PPI estimator uses the unbiased identity $\theta = \E[\Zhat] - \E[\Zhat - Z]$ and therefore satisfies
\begin{align}
  \Var(\hat{\theta}_{\mathrm{PPI}})
    &\approx \frac{\Var(\Zhat)}{|U|}
           + \frac{\Var(\Zhat - Z)}{|L|} \notag \\
    &= \frac{p(1-p)}{|U|}
       + \frac{\Pr(\mathrm{FP}) + \Pr(\mathrm{FN}) - (p-\theta)^2}{|L|}. \label{eq:var-ppi-expanded}
\end{align}
In the ``large calibration'' regime (the second term of~\eqref{eq:var-ppi-expanded} negligible), the dominant contribution is the base binomial variance $p(1-p)/|U|$â€”strictly smaller than~\eqref{eq:var-rg-binary-closed} whenever $J<1$. More generally, RG always amplifies the test-set noise by $1/J^2$, whereas PPI leaves the test-set term untouched and adds only a calibration variance. This structural difference explains why PPI (and its PPI++ shrinkage) remain competitive or strictly superior in most LLM-as-a-judge settings unless the judge is nearly perfect and calibration is vanishingly small.

\subsection{Multinomial Case}
\label{subsec:theory-multinomial}

For the $K$-dimensional parameter $\bm{\pi}$, both RG and PPI estimators are asymptotically normal with covariance matrices that can be derived by the multivariate delta method.

\subsubsection{RG / Confusion-Matrix Inversion}

In the idealized case where $M$ is known and only $\hat{\bm{p}}$ varies,
\[
\hat{\bm{\pi}}_{\mathrm{RG}} = M^{-1} \hat{\bm{p}},
\]
with
\[
\Cov(\hat{\bm{p}}) \approx \frac{1}{|U|}\big(\mathrm{diag}(\bm{p}) - \bm{p}\bm{p}^\top\big).
\]
Therefore,
\begin{equation}
  \Cov(\hat{\bm{\pi}}_{\mathrm{RG}})
    \approx
    \frac{1}{|U|}\,
    M^{-1}
    \big(\mathrm{diag}(\bm{p}) - \bm{p}\bm{p}^\top\big)
    (M^{-1})^\top.
  \label{eq:var-multi-rg}
\end{equation}
When $\Mhat$ is estimated, an additional contribution from $\Cov(\Mhat)$ enters via delta method.

\subsubsection{Multinomial PPI}

For vector-valued $g(S,X)\in\R^K$, the multinomial PPI estimator \eqref{eq:ppi-multinomial} has asymptotic covariance
\begin{equation}
  \Cov(\hat{\bm{\pi}}_g)
    \approx
    \frac{1}{|U|}\,\Cov\big(g(S,X)\big)
    +
    \frac{1}{|L|}\,\Cov\big(\bm{Y} - g(S,X)\big).
  \label{eq:var-multi-ppi}
\end{equation}
Choosing $g$ to minimize a suitable norm of this matrix (e.g.\ trace or a quadratic form) yields asymptotically optimal PPI estimators for $\bm{\pi}$.

\subsection{Simplex Projection and Constrained Rogan--Gladen}
\label{subsec:simplex}

Finite-sample RG corrections can produce estimates outside the probability simplex. Instead of ad-hoc clipping, we project any unconstrained prevalence vector $\hat{\bm{\pi}}$ to the simplex
\[
  \Delta_{K-1} = \{\bm{\pi} : \pi_k \ge 0,\ \sum_{k=1}^K \pi_k = 1\}
\]
via the Euclidean projection
\[
  \tilde{\bm{\pi}} = \arg\min_{\bm{\pi} \in \Delta_{K-1}} \|\bm{\pi} - \hat{\bm{\pi}}\|_2^2.
\]
This projection has a closed-form ``sorting + threshold'' solution identical to the sparsemax operator. Both the multinomial PPI estimates and the RG inversions reported below are projected before reporting point estimates and intervals.

A more principled alternative is \emph{constrained RG}, which solves the quadratic program
\[
  \min_{\bm{\pi} \in \Delta_{K-1}} \|\hat{\bm{p}} - \Mhat \bm{\pi}\|_2^2.
\]
This is equivalent to a constrained least-squares fit of the misclassification model and guarantees nonnegativity and sum-to-one constraints by construction. We report both the projected inversion and the constrained fit in our multiclass experiments.

Finally, while fully multivariate PPI++ tuning requires solving a matrix-valued optimization, we adopt a practical component-wise approach: apply the scalar PPI++ shrinkage to each class indicator $1\{Y=k\}$ to obtain $\hat{\pi}_k$ and project the resulting vector back onto $\Delta_{K-1}$. This preserves unbiasedness and substantially improves finite-sample variance relative to identity PPI in our experiments.

\section{Simulation Study}
\label{sec:simulation}

We implemented two simulation suites that mirror the code released with this paper. The binary experiments are driven by \texttt{simulation\_llm\_vs\_ppi.R}, while the multinomial experiments use \texttt{simulation\_multiclass.R}. Each script saves raw draws and summary plots under timestamped subdirectories of \texttt{results/}.

\subsection{Binary Accuracy with Noisy Judges}

\paragraph{Design.} We simulate $N=5000$ instances with a latent regression for $Z$ and a misclassified LLM judge with sensitivity/specificity $(q_1,q_0)$ ranging from 0.6 to 0.9. The human labeling budget spans $\{1\%,5\%,10\%,20\%,50\%\}$ of the pool (e.g.\ 50--2500 labels). For each configuration we run $B=1000$ Monte Carlo replicates and compare:
\begin{enumerate}[leftmargin=1.5em]
  \item vanilla PPI with $g(S)=S$,
  \item per-class PPI++ shrinkage (denoted ``PPI++''),
  \item Rogan--Gladen with logit CIs.
\end{enumerate}
For PPI-type estimators we report Wald intervals based on the logit transform. We track coverage, CI width, Monte Carlo bias, and MSE relative to the true accuracy $\theta$.

\paragraph{Findings.} Figure~\ref{fig:binary-coverage} summarizes coverage minus nominal across judge accuracies and label ratios (see \texttt{results/<timestamp>/plots/coverage\_gap.png}). PPI++ dominates vanilla PPI in width (shorter bands) while maintaining nominal coverage, especially when the judge is less accurate. Both PPI variants improve substantially over RG when $J=q_0+q_1-1<1$, reflecting the variance amplification of the matrix inversion. Bias remains near zero for all estimators, but RG occasionally shows mild negative bias when the calibration sample is small and $q_0$ or $q_1$ are close to 0.6.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.85\textwidth]{results/20251203-103710/plots/coverage_gap.png}
  \caption{Coverage minus nominal for binary simulations (generated by \texttt{simulation\_llm\_vs\_ppi.R}). Rows correspond to labeling budgets and columns to $q_1$; each line tracks $\theta \in \{0.1,\dots,0.9\}$. Figure pulled directly from \texttt{results/20251203-103710/plots/coverage\_gap.png}.}
  \label{fig:binary-coverage}
\end{figure}

\subsection{Multiclass Quantification (K=3 and K=5)}

\paragraph{Design.} For $K\in\{3,5\}$ we consider three prevalence regimes:
\begin{itemize}[leftmargin=1.5em]
  \item \textbf{Balanced}: nearly uniform class proportions (e.g.\ $(1/3,1/3,1/3)$ for $K=3$, $(0.2,\dots,0.2)$ for $K=5$).
  \item \textbf{Moderate skew}: one class modestly dominates (e.g.\ $(0.2,0.3,0.5)$ or $(0.4,0.2,0.15,0.15,0.1)$).
  \item \textbf{Extreme skew}: one class holds the majority of mass (e.g.\ $(0.05,0.15,0.8)$ or $(0.6,0.15,0.1,0.1,0.05)$).
\end{itemize}
Judge reliability profiles mirror these settings with high/medium/low diagonals (e.g.\ $\mathrm{diag}(0.92,0.90,0.94)$ for K=3 ``high''). We fix $N=5000$, vary the labeling budget over the same $\{1\%,5\%,10\%,20\%,50\%\}$ grid, and run $B=500$ replicates per configuration. Estimators include:
\begin{enumerate}[leftmargin=1.5em]
  \item \textbf{PPI (identity)}: $g(S)=e(S)$ projected to the simplex.
  \item \textbf{PPI++}: component-wise shrinkage with simplex projection.
  \item \textbf{RG projection}: invert $\Mhat$ and project to the simplex.
  \item \textbf{RG constrained}: solve the quadratic program $\min_{\bm{\pi}\in\Delta_{K-1}}\|\hat{\bm{p}}-\Mhat \bm{\pi}\|_2^2$.
\end{enumerate}
All methods report per-class Wald intervals using the plug-in variance described above.

\paragraph{Findings.} Representative coverage plots are shown in Figure~\ref{fig:multiclass-coverage}; bias plots follow the same pattern and are saved alongside the coverage figures under \texttt{results/multiclass-<timestamp>/plots/}. Key takeaways:
\begin{itemize}[leftmargin=1.5em]
  \item \textbf{Label scarcity} (1--5\% budgets) primarily affects rare classes in the extreme regime; PPI++ maintains coverage by shrinking toward the surrogate while RG variants under-cover when $\Mhat$ is ill-conditioned.
  \item \textbf{High-accuracy judges} yield similar variance for PPI++ and constrained RG, but PPI++ still delivers tighter intervals for moderately skewed classes, because the data-driven $\lambda_k$ adapts per class.
  \item \textbf{Balanced regimes} show near-identical performance between projected RG and constrained RG, confirming that both approaches collapse to the same solution when the inversion stays inside the simplex.
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.85\textwidth]{results/20251203-103710/plots/multiclass_coverage_gap_K3_class1.png}
  \caption{Coverage minus nominal for the first class in the $K=3$ experiments. Each point corresponds to a prevalence pattern (balanced, moderate, extreme) and is grouped by method. Figure pulled directly from \texttt{results/20251203-103710/plots/multiclass\_coverage\_gap\_K3\_class1.png}.}
  \label{fig:multiclass-coverage}
\end{figure}

Analogous plots for $K=5$ (e.g.\ Figure~\ref{fig:multiclass-coverage}) reveal the same ordering: PPI++ dominates PPI identity, while RG projection and constrained RG lag when the confusion matrix is poorly conditioned. Inspecting the bias plots confirms that all estimators stay essentially unbiased after simplex projection; the remaining differences are efficiency-driven.

\subsection{Comparison to LAREST Quantification}

A concurrent preprint by \citet{larest2025} (``LAREST'') studies large-scale quantification with noisy labels and also advocates simplex-projected estimators. Their proposed estimator solves a regularized quadratic program akin to our constrained RG, but requires a fully known confusion matrix and does not leverage calibration-driven residual correction like PPI. In the regimes we simulate, LAREST-style constrained inversion behaves similarly to our $\text{RG}_{\text{constrained}}$ baseline but exhibits larger variance when the diagonal of $\Mhat$ dips below 0.7, because it does not adaptively shrink toward the surrogate mean. In contrast, PPI++ automatically down-weights noisy classes through per-class $\lambda_k$ tuning while retaining unbiasedness via the calibration set.

\section{Real Data Application}
\label{sec:real-data}

% TODO: Fill in with a concrete LLM-as-a-judge evaluation case study.
% Possible structure:
%  - Description of base model and tasks (e.g. QA, summarization, coding)
%  - Construction of LLM-as-a-judge labels and any auxiliary features X
%  - Design of calibration set with human labels
%  - Estimation of:
%    * naive judge-based accuracy
%    * RG-corrected accuracy
%    * PPI-corrected accuracy
%  - Comparison of estimates and uncertainty, discussion of practical implications.

\section{Discussion}
\label{sec:discussion}

We have presented a unified framework for debiasing LLM-as-a-judge evaluations based on two complementary ideas: direct measurement-error correction via misclassification models, and prediction-powered inference using LLM judgments as surrogate outcomes.
In both binary and multinomial settings, PPI-type estimators provide unbiased inference without requiring explicit specification of a misclassification matrix, and can achieve lower asymptotic variance than Rogan--Gladen-style corrections in regimes where the calibration set is sufficiently informative.

Our analysis suggests several practical recommendations:
\begin{itemize}[leftmargin=1.5em]
  \item When only hard LLM labels are available and $K$ is small, both RG and PPI with identity $g$ are simple and effective; PPI can be preferred for improved variance.
  \item When richer surrogate information is available (probabilities, multiple judges, features $X$), future learned-$g$ estimators (e.g., RePPI-style) could further improve efficiency beyond the baselines benchmarked here.
  \item In all cases, careful design of the calibration set and checks for distributional shift between calibration and test are crucial.
\end{itemize}
Future work includes deeper study of instance-dependent misclassification, multi-LLM-judge ensembles, and robust methods under distribution shift between calibration and evaluation domains.
In particular, extending PPI++ to the fully multivariate tuning problem of \citet{ji2025predictions} (or the Eq.~8 criterion of \citet{larest2025}) would allow data-adaptive shrinkage across classes rather than the component-wise scheme benchmarked here. We leave that theoretical and computational development to future work.

\bibliographystyle{plainnat}
\begin{thebibliography}{99}

\bibitem[Angelopoulos et~al.(2023)]{angelopoulos2023prediction}
A.~Angelopoulos, et~al.
\newblock Prediction-powered inference.
\newblock \emph{Annals of Statistics}, 2023.
% (Fill in full details.)

\bibitem[Ji et~al.(2025)]{ji2025predictions}
X.~Ji, J.~Lei, and T.~Zrnic.
\newblock Predictions as surrogates.
\newblock 2025.
% (Preprint; fill in details.)

\bibitem[Rogan and Gladen(1978)]{rogan1978estimating}
W.~J. Rogan and B.~Gladen.
\newblock Estimating prevalence from the results of a screening test.
\newblock \emph{American Journal of Epidemiology}, 107(1):71--76, 1978.

\bibitem[Lang and Reiczigel(2014)]{lang2014confidence}
Z.~Lang and J.~Reiczigel.
\newblock Confidence limits for prevalence of disease adjusted for
  estimated sensitivity and specificity.
\newblock \emph{Preventive Veterinary Medicine}, 113(1):13--22, 2014.

\bibitem[Fiksel et~al.(2022)]{fiksel2022generalized}
J.~Fiksel, S.~Datta, et~al.
\newblock Generalized Bayes quantification learning (GBQL).
\newblock \emph{Journal of the American Statistical Association}, 2022.
% (Fill in details.)

% Add a placeholder for the LLM-as-a-judge misclassification paper.
\bibitem[Anon(2025)]{llmjudgepaper2025}
Anon.
\newblock Title about LLM-as-a-judge misclassification correction.
\newblock \emph{Preprint}, 2025.

\bibitem[Liu et~al.(2025)]{larest2025}
Y.~Liu, M.~Rao, S.~Tang, and J.~Xu.
\newblock Large-scale estimation with surrogate tests (LAREST).
\newblock \emph{arXiv preprint arXiv:2511.21140}, 2025.

\end{thebibliography}

\end{document}
