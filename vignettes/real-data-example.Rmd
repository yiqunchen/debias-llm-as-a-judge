---
title: "Real Data Example: GPT-4o-mini as Judge"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Real Data Example: GPT-4o-mini as Judge}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5
)
```

## Overview

This vignette demonstrates applying debiasing methods to real LLM-as-a-judge data. We use GPT-4o-mini as a judge to evaluate pairwise comparisons between Claude Opus 4 and Gemini 2.5 Pro, comparing judge decisions against human preferences from Chatbot Arena.

## Setup

```{r setup, message=FALSE}
library(debiasLLMReporting)
library(jsonlite)
library(dplyr)
library(ggplot2)

set.seed(2024)
```

## Load Data

The package includes example judge results from GPT-4o-mini evaluating Claude Opus 4 vs Gemini 2.5 Pro comparisons:

```{r load-data}
# Load from package extdata
json_path <- system.file("extdata", "example_judge_data.json",
                         package = "debiasLLMReporting")
judge_data <- fromJSON(json_path)

head(judge_data)
```

## Prepare Data

The data contains pairwise comparisons where model positions (A vs B) are randomized. We need to normalize to consistently measure Claude's win rate:

```{r prepare-data}
df <- judge_data %>%
  filter(!is.na(judge_pick)) %>%
  mutate(
    # Identify which position Claude is in
    claude_is_a = (model_a == "claude-opus-4-20250514"),
    # Y = 1 if human preferred Claude, 0 otherwise
    Y = case_when(
      claude_is_a & human_winner == "model_a" ~ 1L,
      !claude_is_a & human_winner == "model_b" ~ 1L,
      TRUE ~ 0L
    ),
    # Yhat = 1 if judge picked Claude, 0 otherwise
    Yhat = case_when(
      claude_is_a & judge_pick == "A" ~ 1L,
      !claude_is_a & judge_pick == "B" ~ 1L,
      TRUE ~ 0L
    )
  )

cat("Total comparisons:", nrow(df), "\n")
cat("Human preference for Claude:", round(mean(df$Y), 3), "\n")
cat("Judge preference for Claude:", round(mean(df$Yhat), 3), "\n")
```

## Split into Calibration and Test Sets

We use 10% of data as the labeled calibration set (where we have both human and judge labels). In practice, only the calibration set would require expensive human annotation.

```{r split-data}
LABEL_RATIO <- 0.10
N <- nrow(df)
m <- max(4L, round(LABEL_RATIO * N))
n <- N - m

# Random split
idx_cal <- sample(N, m)
idx_test <- setdiff(seq_len(N), idx_cal)

Y_cal <- df$Y[idx_cal]
Yhat_cal <- df$Yhat[idx_cal]
Yhat_test <- df$Yhat[idx_test]

cat("Calibration set size:", m, "(", round(100*m/N, 1), "% )\n")
cat("Test set size:", n, "\n")
```

## Estimate Confusion Matrix

From the calibration set, we estimate the judge's sensitivity and specificity:

```{r confusion-matrix}
m0 <- sum(Y_cal == 0)
m1 <- sum(Y_cal == 1)

# Specificity: P(judge says not-Claude | human says not-Claude)
q0_hat <- mean(Yhat_cal[Y_cal == 0] == 0)

# Sensitivity: P(judge says Claude | human says Claude)
q1_hat <- mean(Yhat_cal[Y_cal == 1] == 1)

# Positive rate in test set
p_hat <- mean(Yhat_test)

cat("Estimated specificity (q0):", round(q0_hat, 3), "\n")
cat("Estimated sensitivity (q1):", round(q1_hat, 3), "\n")
cat("Test set positive rate:", round(p_hat, 3), "\n")
```

## Apply All Estimators

```{r apply-estimators}
ALPHA <- 0.10  # 90% confidence intervals

# Ground truth (using all human labels - in practice unknown for test set)
theta_true <- mean(df$Y)

# 1. Naive (no correction)
naive_theta <- p_hat
naive_var <- p_hat * (1 - p_hat) / n
z_alpha <- qnorm(1 - ALPHA / 2)
naive_ci <- c(
  pmax(naive_theta - z_alpha * sqrt(naive_var), 0),
  pmin(naive_theta + z_alpha * sqrt(naive_var), 1)
)

# 2. PPI
ppi_result <- ppi_point_and_ci(
  Y_L = Y_cal,
  f_L = Yhat_cal,
  f_U = Yhat_test,
  alpha = ALPHA
)

# 3. PPI++
ppi_pp_result <- ppi_pp_point_and_ci_general(
  Y_L = Y_cal,
  f_L = Yhat_cal,
  f_U = Yhat_test,
  alpha = ALPHA
)

# 4. Rogan-Gladen
rg_result <- llm_point_and_ci(
  p_hat = p_hat,
  q0_hat = q0_hat,
  q1_hat = q1_hat,
  n = n,
  m0 = m0,
  m1 = m1,
  alpha = ALPHA
)

# 5. EIF (Efficient Influence Function)
eif_result <- eif_point_and_ci(
  Y_cal = Y_cal,
  Yhat_cal = Yhat_cal,
  Yhat_test = Yhat_test,
  alpha = ALPHA
)
```

## Results Summary

```{r results-table}
results <- tibble(
  Method = c("True", "Naive", "PPI", "PPI++", "Rogan-Gladen", "EIF"),
  Estimate = c(
    theta_true,
    naive_theta,
    ppi_result$theta,
    ppi_pp_result$theta,
    rg_result$theta,
    eif_result$theta
  ),
  CI_Lower = c(NA, naive_ci[1], ppi_result$ci_lower, ppi_pp_result$ci_lower,
               rg_result$ci_lower, eif_result$ci_lower),
  CI_Upper = c(NA, naive_ci[2], ppi_result$ci_upper, ppi_pp_result$ci_upper,
               rg_result$ci_upper, eif_result$ci_upper)
) %>%
  mutate(
    Bias = round(Estimate - theta_true, 4),
    CI_Width = round(CI_Upper - CI_Lower, 3),
    Covers = CI_Lower <= theta_true & CI_Upper >= theta_true
  )

knitr::kable(results, digits = 3, caption = "Estimator Comparison: Claude Opus 4 Win Rate")
```

## Visualization

```{r plot-results, fig.height=5}
METHOD_COLORS <- c(
  "Naive" = "#525252",
  "PPI" = "#1B9E77",
  "PPI++" = "#D95F02",
  "Rogan-Gladen" = "#7570B3",
  "EIF" = "#E6AB02"
)

plot_df <- results %>%
  filter(Method != "True") %>%
  mutate(Method = factor(Method, levels = names(METHOD_COLORS)))

ggplot(plot_df, aes(x = Method, y = Estimate, color = Method)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper),
                width = 0.2, linewidth = 1.2) +
  geom_hline(yintercept = theta_true, linetype = "solid",
             color = "black", linewidth = 1) +
  annotate("text", x = 0.55, y = theta_true + 0.025,
           label = paste("True =", round(theta_true, 3)),
           hjust = 0, fontface = "bold") +
  scale_color_manual(values = METHOD_COLORS) +
  labs(
    title = "Claude Opus 4 vs Gemini 2.5 Pro Win Rate",
    subtitle = paste0("Judge: GPT-4o-mini | ", round(100*m/N),
                      "% labeled | ", 100*(1-ALPHA), "% CI"),
    x = NULL,
    y = "Estimated Win Rate (Claude)"
  ) +
  theme_bw(base_size = 12) +
  theme(
    legend.position = "none",
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5),
    axis.text.x = element_text(angle = 30, hjust = 1)
  ) +
  coord_cartesian(ylim = c(0, 1))
```

## Interpretation

- **Naive estimate**: Uses only judge predictions without correction. May be biased if GPT-4o-mini has systematic preferences.

- **PPI/PPI++**: Uses the calibration residuals to correct for systematic differences between judge and human preferences.

- **Rogan-Gladen**: Classical misclassification correction using estimated sensitivity/specificity.

- **EIF**: Efficient influence function estimator that achieves semiparametric efficiency.

The debiased estimates should be closer to the true human preference rate, with confidence intervals that provide valid coverage even when the judge is imperfect.

## Bootstrap Confidence Intervals (Optional)

For more robust inference with small calibration sets:

```{r bootstrap, eval=FALSE}
# PPI with bootstrap
ppi_boot <- ppi_bootstrap(
  Y_L = Y_cal,
  f_L = Yhat_cal,
  f_U = Yhat_test,
  B = 500,
  alpha = ALPHA
)

# Rogan-Gladen with bootstrap
rg_boot <- rg_bootstrap(
  Y_cal = Y_cal,
  Yhat_cal = Yhat_cal,
  Yhat_test = Yhat_test,
  B = 500,
  alpha = ALPHA
)
```

## Session Info

```{r session-info}
sessionInfo()
```
